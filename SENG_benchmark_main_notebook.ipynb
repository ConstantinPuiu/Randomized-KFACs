{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3466,
     "status": "ok",
     "timestamp": 1654249876037,
     "user": {
      "displayName": "Constantin Puiu",
      "userId": "15538331899008717371"
     },
     "user_tz": -60
    },
    "id": "fpp4PfYe-5Ig",
    "outputId": "62900073-7ef2-4149-96fb-4f89148cc886"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "import sys\n",
    "sys.path.append('/content/gdrive/My Drive/Colab Notebooks/Lib_files/SENG_lib_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1654249876038,
     "user": {
      "displayName": "Constantin Puiu",
      "userId": "15538331899008717371"
     },
     "user_tz": -60
    },
    "id": "ckRKD0vrIGFh",
    "outputId": "9b5f966e-1a01-491b-a3b5-075d8d15aec7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' DEBUG'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' DEBUG'''\n",
    "#!pip install -Uqq ipdb\n",
    "#import ipdb\n",
    "#%pdb on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1654249876039,
     "user": {
      "displayName": "Constantin Puiu",
      "userId": "15538331899008717371"
     },
     "user_tz": -60
    },
    "id": "U7PvGkQxz6I_",
    "outputId": "f191b7a9-5bb2-47f7-9cf4-8aa510149e8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun  3 09:51:15 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   38C    P0    23W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# GPU bit!\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 889,
     "status": "ok",
     "timestamp": 1654249876919,
     "user": {
      "displayName": "Constantin Puiu",
      "userId": "15538331899008717371"
     },
     "user_tz": -60
    },
    "id": "4xdyCPdJFrgK",
    "outputId": "723aed4d-3222-42a8-a0af-8c8699ead330"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Tesla V100-SXM2-16GB\n"
     ]
    }
   ],
   "source": [
    "#GPU 2\n",
    "import torch\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "cuda0 = torch.device('cuda:0')#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1654249876921,
     "user": {
      "displayName": "Constantin Puiu",
      "userId": "15538331899008717371"
     },
     "user_tz": -60
    },
    "id": "R4LfkVcE2fb1",
    "outputId": "8fafe32a-9d7b-4f9b-aeba-8993c9141fce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your runtime has 54.8 gigabytes of available RAM\n",
      "\n",
      "You are using a high-RAM runtime!\n"
     ]
    }
   ],
   "source": [
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8394252,
     "status": "ok",
     "timestamp": 1654258271159,
     "user": {
      "displayName": "Constantin Puiu",
      "userId": "15538331899008717371"
     },
     "user_tz": -60
    },
    "id": "hREsm1ih5Io-",
    "outputId": "278cc267-3b32-4474-b6e6-f6c0e082ff5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing seed 102\n",
      "\n",
      "Use GPU: 0 for training\n",
      "==> Running with ['/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py', '-f', '/root/.local/share/jupyter/runtime/kernel-84a8d6fb-b2d8-40be-b67f-a96c7eebbe81.json']\n",
      "==> Building model..\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/Colab Notebooks/Lib_files/SENG_lib_files/seng.py:415: UserWarning: An output with one or more elements was resized since it had shape [1180160], which does not match the required output shape [512, 2305].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)\n",
      "  return torch.mm(gy_v, x_v.t(), out=out).view(-1)\n",
      "/content/gdrive/My Drive/Colab Notebooks/Lib_files/SENG_lib_files/seng.py:415: UserWarning: An output with one or more elements was resized since it had shape [2359808], which does not match the required output shape [512, 4609].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)\n",
      "  return torch.mm(gy_v, x_v.t(), out=out).view(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory peak: 4582960640 Bytes\n",
      "Epoch  testloss  testacc  trainloss  trainacc  time\n",
      "   1   1.9403     35.540  1.6519    37.294  17.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/Colab Notebooks/Lib_files/SENG_lib_files/seng.py:285: UserWarning: An output with one or more elements was resized since it had shape [256, 1792], which does not match the required output shape [256, 64, 28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)\n",
      "  dw = torch.bmm(gy, x.transpose(1, 2), out=dw)\n",
      "/content/gdrive/My Drive/Colab Notebooks/Lib_files/SENG_lib_files/seng.py:285: UserWarning: An output with one or more elements was resized since it had shape [256, 36928], which does not match the required output shape [256, 64, 577].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)\n",
      "  dw = torch.bmm(gy, x.transpose(1, 2), out=dw)\n",
      "/content/gdrive/My Drive/Colab Notebooks/Lib_files/SENG_lib_files/seng.py:285: UserWarning: An output with one or more elements was resized since it had shape [256, 73856], which does not match the required output shape [256, 128, 577].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)\n",
      "  dw = torch.bmm(gy, x.transpose(1, 2), out=dw)\n",
      "/content/gdrive/My Drive/Colab Notebooks/Lib_files/SENG_lib_files/seng.py:285: UserWarning: An output with one or more elements was resized since it had shape [256, 147584], which does not match the required output shape [256, 128, 1153].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)\n",
      "  dw = torch.bmm(gy, x.transpose(1, 2), out=dw)\n",
      "/content/gdrive/My Drive/Colab Notebooks/Lib_files/SENG_lib_files/seng.py:285: UserWarning: An output with one or more elements was resized since it had shape [256, 295168], which does not match the required output shape [256, 256, 1153].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)\n",
      "  dw = torch.bmm(gy, x.transpose(1, 2), out=dw)\n",
      "/content/gdrive/My Drive/Colab Notebooks/Lib_files/SENG_lib_files/seng.py:285: UserWarning: An output with one or more elements was resized since it had shape [256, 590080], which does not match the required output shape [256, 256, 2305].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)\n",
      "  dw = torch.bmm(gy, x.transpose(1, 2), out=dw)\n",
      "/content/gdrive/My Drive/Colab Notebooks/Lib_files/SENG_lib_files/seng.py:285: UserWarning: An output with one or more elements was resized since it had shape [256, 262656], which does not match the required output shape [256, 512, 513].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)\n",
      "  dw = torch.bmm(gy, x.transpose(1, 2), out=dw)\n",
      "/content/gdrive/My Drive/Colab Notebooks/Lib_files/SENG_lib_files/seng.py:285: UserWarning: An output with one or more elements was resized since it had shape [256, 5130], which does not match the required output shape [256, 10, 513].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)\n",
      "  dw = torch.bmm(gy, x.transpose(1, 2), out=dw)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2   1.4006     49.230  1.1412    60.182  34.39\n",
      "   3   1.0407     67.490  0.9915    66.848  50.87\n",
      "   4   1.2049     59.190  0.9253    69.842  67.58\n",
      "   5   1.1798     60.300  0.8684    72.130  84.37\n",
      "   6   1.0889     65.780  0.8181    74.276  101.09\n",
      "   7   1.0641     65.070  0.7764    75.602  118.15\n",
      "   8   1.3369     59.790  0.7395    76.718  135.00\n",
      "   9   0.9964     67.720  0.7137    77.886  151.88\n",
      "  10   0.9630     69.030  0.6914    78.756  168.46\n",
      "  11   0.7983     73.660  0.6596    79.666  185.18\n",
      "  12   0.6816     78.230  0.6324    80.534  201.90\n",
      "  13   0.7895     74.540  0.6021    81.420  218.55\n",
      "  14   0.8143     74.080  0.5867    81.882  235.33\n",
      "  15   0.6318     80.480  0.5619    83.020  252.21\n",
      "  16   0.8030     74.800  0.5331    84.046  268.82\n",
      "  17   1.0563     65.320  0.5139    84.488  285.44\n",
      "  18   0.6731     79.270  0.4828    85.646  302.41\n",
      "  19   1.1805     64.440  0.4658    86.174  319.34\n",
      "  20   0.7539     75.710  0.4386    87.050  336.02\n",
      "  21   0.8183     74.970  0.4150    87.904  352.74\n",
      "  22   0.7322     76.920  0.3937    88.526  369.50\n",
      "  23   0.8282     74.010  0.3736    89.128  386.28\n",
      "  24   1.3418     60.020  0.3567    89.912  402.92\n",
      "  25   0.8628     72.890  0.3331    90.588  419.48\n",
      "  26   0.6505     80.240  0.3076    91.542  436.28\n",
      "  27   1.3674     60.690  0.2886    92.144  453.68\n",
      "  28   1.3706     61.750  0.2722    92.624  470.39\n",
      "  29   0.5155     84.740  0.2519    93.332  486.93\n",
      "  30   0.6468     79.100  0.2349    93.940  503.36\n",
      "  31   0.9438     72.440  0.2138    94.734  520.25\n",
      "  32   0.4151     87.790  0.1981    95.238  537.29\n",
      "  33   0.7120     78.520  0.1756    95.950  553.77\n",
      "  34   0.4220     87.800  0.1646    96.328  570.30\n",
      "  35   0.4658     86.030  0.1504    96.892  586.85\n",
      "  36   0.8614     76.030  0.1367    97.306  603.58\n",
      "  37   0.5015     85.620  0.1258    97.708  620.13\n",
      "  38   0.4870     86.560  0.1120    98.256  637.19\n",
      "  39   0.4455     87.620  0.1019    98.518  654.11\n",
      "  40   0.4261     88.170  0.0958    98.708  670.79\n",
      "  41   0.3284     91.300  0.0886    98.956  687.28\n",
      "  42   0.3805     89.710  0.0852    99.030  703.98\n",
      "  43   0.2883     92.180  0.0792    99.234  720.83\n",
      "  44   0.2824     92.510  0.0773    99.316  737.90\n",
      "  45   0.2956     92.070  0.0739    99.414  754.48\n",
      "  46   0.2684     92.790  0.0738    99.388  771.26\n",
      "  47   0.2684     92.700  0.0714    99.500  788.43\n",
      "  48   0.2672     92.730  0.0715    99.476  805.03\n",
      "  49   0.2614     92.910  0.0710    99.512  821.76\n",
      "  50   0.2653     92.730  0.0685    99.596  838.28\n",
      "Doing seed 103\n",
      "\n",
      "Use GPU: 0 for training\n",
      "==> Running with ['/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py', '-f', '/root/.local/share/jupyter/runtime/kernel-84a8d6fb-b2d8-40be-b67f-a96c7eebbe81.json']\n",
      "==> Building model..\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Memory peak: 4582960640 Bytes\n",
      "Epoch  testloss  testacc  trainloss  trainacc  time\n",
      "   1   1.8229     35.360  1.6586    36.976  16.62\n",
      "   2   1.2152     61.590  1.1303    60.494  33.05\n",
      "   3   1.0126     68.290  0.9795    67.246  49.46\n",
      "   4   1.1191     64.160  0.9254    70.162  66.02\n",
      "   5   0.9049     70.680  0.8714    72.224  82.86\n",
      "   6   1.3142     58.000  0.8146    74.270  99.74\n",
      "   7   1.0748     66.520  0.7798    75.440  116.44\n",
      "   8   1.0051     68.260  0.7461    76.786  132.95\n",
      "   9   0.9331     68.740  0.7195    77.458  149.51\n",
      "  10   1.0332     67.190  0.6886    78.552  166.28\n",
      "  11   0.9438     68.040  0.6504    79.946  182.72\n",
      "  12   1.2532     62.990  0.6311    80.694  199.20\n",
      "  13   0.7183     77.270  0.6123    81.338  215.78\n",
      "  14   0.8496     71.780  0.5707    82.644  232.17\n",
      "  15   0.8501     72.280  0.5620    82.988  248.81\n",
      "  16   0.7103     76.910  0.5312    83.980  265.51\n",
      "  17   0.7472     76.980  0.5051    84.978  282.59\n",
      "  18   0.7517     75.290  0.4801    85.814  299.18\n",
      "  19   0.6412     81.310  0.4655    86.292  315.80\n",
      "  20   1.5683     54.530  0.4431    87.080  332.17\n",
      "  21   0.7720     75.500  0.4143    87.840  348.61\n",
      "  22   1.1346     65.830  0.4004    88.560  365.12\n",
      "  23   0.6354     80.100  0.3748    89.394  381.60\n",
      "  24   0.4976     83.860  0.3557    89.960  398.08\n",
      "  25   1.0321     69.850  0.3376    90.484  414.53\n",
      "  26   0.5515     82.930  0.3078    91.518  431.26\n",
      "  27   0.7611     77.130  0.2947    91.928  447.83\n",
      "  28   0.7932     74.870  0.2703    92.836  464.28\n",
      "  29   0.9332     72.960  0.2507    93.388  481.17\n",
      "  30   0.7266     77.370  0.2331    94.066  497.83\n",
      "  31   1.0352     70.410  0.2143    94.612  514.09\n",
      "  32   0.7775     76.370  0.1973    95.222  530.45\n",
      "  33   0.6050     82.020  0.1767    95.910  546.94\n",
      "  34   0.9740     71.210  0.1659    96.276  563.31\n",
      "  35   0.6036     82.760  0.1460    96.996  579.59\n",
      "  36   0.7182     79.490  0.1357    97.276  596.33\n",
      "  37   0.6192     82.690  0.1230    97.814  613.11\n",
      "  38   0.4048     88.650  0.1124    98.148  629.77\n",
      "  39   0.4281     88.040  0.1041    98.394  646.02\n",
      "  40   0.3677     89.500  0.0969    98.638  662.77\n",
      "  41   0.2967     91.940  0.0921    98.832  679.26\n",
      "  42   0.3261     91.150  0.0845    99.070  695.68\n",
      "  43   0.3390     90.700  0.0799    99.260  712.06\n",
      "  44   0.2846     92.540  0.0777    99.286  728.86\n",
      "  45   0.2715     92.740  0.0761    99.358  745.46\n",
      "  46   0.2672     92.940  0.0718    99.464  762.03\n",
      "  47   0.2669     93.030  0.0714    99.534  778.49\n",
      "  48   0.2716     92.840  0.0694    99.534  794.95\n",
      "  49   0.2686     93.050  0.0697    99.536  811.35\n",
      "  50   0.2673     92.920  0.0683    99.608  827.70\n",
      "Doing seed 104\n",
      "\n",
      "Use GPU: 0 for training\n",
      "==> Running with ['/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py', '-f', '/root/.local/share/jupyter/runtime/kernel-84a8d6fb-b2d8-40be-b67f-a96c7eebbe81.json']\n",
      "==> Building model..\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Memory peak: 4593393152 Bytes\n",
      "Epoch  testloss  testacc  trainloss  trainacc  time\n",
      "   1   1.6206     43.550  1.6503    37.240  16.51\n",
      "   2   1.3720     53.760  1.1145    61.090  32.97\n",
      "   3   1.2995     53.990  0.9795    67.386  49.28\n",
      "   4   0.9282     70.400  0.9303    69.708  66.13\n",
      "   5   1.2437     60.820  0.8633    72.472  82.60\n",
      "   6   1.0612     64.430  0.8158    74.454  99.75\n",
      "   7   1.0230     67.320  0.7851    75.262  116.40\n",
      "   8   0.7974     75.050  0.7441    76.704  132.93\n",
      "   9   0.8890     70.660  0.7199    77.480  149.55\n",
      "  10   0.8024     74.120  0.6824    78.792  165.99\n",
      "  11   0.7923     74.640  0.6578    79.698  182.56\n",
      "  12   1.1417     64.180  0.6425    80.052  198.98\n",
      "  13   0.7992     73.980  0.6090    81.336  215.47\n",
      "  14   0.7680     75.500  0.5808    82.332  231.84\n",
      "  15   1.0180     67.090  0.5605    82.984  248.37\n",
      "  16   0.7569     74.810  0.5370    83.870  264.59\n",
      "  17   0.7251     77.050  0.5121    84.702  280.94\n",
      "  18   0.7915     74.890  0.4830    85.464  297.47\n",
      "  19   0.9156     70.180  0.4614    86.318  314.07\n",
      "  20   0.8182     74.650  0.4409    86.984  331.01\n",
      "  21   0.7792     75.480  0.4132    87.902  347.56\n",
      "  22   0.9778     68.940  0.3992    88.378  364.20\n",
      "  23   1.4930     55.170  0.3749    89.310  381.38\n",
      "  24   0.7058     77.870  0.3521    90.044  398.45\n",
      "  25   1.2823     63.700  0.3288    90.820  415.61\n",
      "  26   0.7723     76.600  0.3157    91.236  433.20\n",
      "  27   0.8317     73.420  0.2889    92.100  450.22\n",
      "  28   0.5946     81.180  0.2707    92.762  466.97\n",
      "  29   0.6430     80.750  0.2556    93.186  483.85\n",
      "  30   0.8817     73.170  0.2334    94.036  500.65\n",
      "  31   0.8020     76.240  0.2139    94.658  517.39\n",
      "  32   0.9287     73.300  0.1955    95.356  534.32\n",
      "  33   0.5257     83.890  0.1831    95.746  551.24\n",
      "  34   0.8780     74.880  0.1687    96.234  567.98\n",
      "  35   0.7216     78.510  0.1532    96.746  584.69\n",
      "  36   0.5684     83.020  0.1350    97.466  601.13\n",
      "  37   0.5198     85.260  0.1223    97.786  617.68\n",
      "  38   0.7352     78.920  0.1113    98.260  634.38\n",
      "  39   0.2963     91.710  0.1045    98.414  650.72\n",
      "  40   0.4268     87.770  0.0972    98.638  667.51\n",
      "  41   0.5230     85.460  0.0913    98.862  684.02\n",
      "  42   0.3127     91.190  0.0857    99.004  700.59\n",
      "  43   0.2967     91.850  0.0818    99.156  717.17\n",
      "  44   0.2689     92.790  0.0772    99.342  733.61\n",
      "  45   0.2871     92.290  0.0751    99.394  750.21\n",
      "  46   0.2615     93.020  0.0728    99.474  767.55\n",
      "  47   0.2724     92.870  0.0720    99.498  784.30\n",
      "  48   0.2687     92.940  0.0713    99.484  800.85\n",
      "  49   0.2581     93.160  0.0702    99.534  817.18\n",
      "  50   0.2564     93.170  0.0692    99.566  833.77\n",
      "Doing seed 105\n",
      "\n",
      "Use GPU: 0 for training\n",
      "==> Running with ['/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py', '-f', '/root/.local/share/jupyter/runtime/kernel-84a8d6fb-b2d8-40be-b67f-a96c7eebbe81.json']\n",
      "==> Building model..\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Memory peak: 4593393152 Bytes\n",
      "Epoch  testloss  testacc  trainloss  trainacc  time\n",
      "   1   1.8157     35.260  1.6546    37.176  16.52\n",
      "   2   1.4979     48.820  1.1387    60.360  32.96\n",
      "   3   1.0727     65.510  0.9903    66.892  49.88\n",
      "   4   1.1391     63.290  0.9219    69.744  66.35\n",
      "   5   1.0218     66.070  0.8854    71.638  82.78\n",
      "   6   0.8669     71.330  0.8329    73.782  99.12\n",
      "   7   1.2517     60.310  0.7802    75.344  115.65\n",
      "   8   1.0358     67.440  0.7489    76.518  132.52\n",
      "   9   2.1916     45.230  0.7167    77.712  149.41\n",
      "  10   0.8215     74.630  0.6958    78.426  165.90\n",
      "  11   0.8105     73.810  0.6733    79.210  182.31\n",
      "  12   0.8537     71.790  0.6261    80.826  198.94\n",
      "  13   0.8190     73.280  0.6084    81.372  215.49\n",
      "  14   0.9638     69.560  0.5786    82.224  232.10\n",
      "  15   0.7311     76.590  0.5567    83.074  249.00\n",
      "  16   0.9598     70.410  0.5302    84.168  266.10\n",
      "  17   0.8455     71.730  0.5039    84.988  282.66\n",
      "  18   0.6760     78.520  0.4851    85.598  299.00\n",
      "  19   0.8973     71.820  0.4668    86.038  315.72\n",
      "  20   0.5918     81.250  0.4363    87.132  332.50\n",
      "  21   0.6516     79.980  0.4231    87.664  349.09\n",
      "  22   0.8278     73.340  0.3950    88.534  365.93\n",
      "  23   1.0284     68.140  0.3729    89.328  382.59\n",
      "  24   0.7029     77.890  0.3566    89.846  399.10\n",
      "  25   0.7719     75.820  0.3334    90.714  416.15\n",
      "  26   0.5499     82.580  0.3195    91.144  432.87\n",
      "  27   0.9521     72.150  0.2987    91.794  449.54\n",
      "  28   0.6646     79.440  0.2718    92.682  466.10\n",
      "  29   1.3721     61.550  0.2552    93.344  482.41\n",
      "  30   1.4241     57.440  0.2331    94.088  498.79\n",
      "  31   0.6420     80.050  0.2150    94.656  515.13\n",
      "  32   0.7076     79.210  0.1982    95.264  531.46\n",
      "  33   1.0847     68.170  0.1854    95.534  547.74\n",
      "  34   0.5336     83.520  0.1618    96.504  564.03\n",
      "  35   0.4449     86.970  0.1506    96.854  580.46\n",
      "  36   0.5444     84.550  0.1373    97.278  597.11\n",
      "  37   0.7296     80.860  0.1239    97.764  613.05\n",
      "  38   0.7675     78.250  0.1153    97.994  629.42\n",
      "  39   0.5774     84.000  0.1046    98.414  645.67\n",
      "  40   0.3764     89.370  0.0953    98.706  661.77\n",
      "  41   0.3125     91.390  0.0908    98.864  677.93\n",
      "  42   0.2864     92.020  0.0854    99.044  694.26\n",
      "  43   0.3106     91.290  0.0801    99.202  710.52\n",
      "  44   0.2936     91.910  0.0774    99.278  727.51\n",
      "  45   0.2945     91.850  0.0745    99.410  743.86\n",
      "  46   0.2727     92.500  0.0736    99.422  760.09\n",
      "  47   0.2759     92.600  0.0727    99.452  776.17\n",
      "  48   0.2695     92.700  0.0708    99.530  792.49\n",
      "  49   0.2742     92.600  0.0694    99.544  808.67\n",
      "  50   0.2724     92.430  0.0701    99.538  824.67\n",
      "Doing seed 106\n",
      "\n",
      "Use GPU: 0 for training\n",
      "==> Running with ['/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py', '-f', '/root/.local/share/jupyter/runtime/kernel-84a8d6fb-b2d8-40be-b67f-a96c7eebbe81.json']\n",
      "==> Building model..\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Memory peak: 4593393152 Bytes\n",
      "Epoch  testloss  testacc  trainloss  trainacc  time\n",
      "   1   2.1564     24.490  1.6585    36.898  16.35\n",
      "   2   1.5979     44.550  1.1331    60.258  32.56\n",
      "   3   1.1052     65.530  0.9899    66.658  48.76\n",
      "   4   1.3104     55.360  0.9370    69.042  65.15\n",
      "   5   1.1055     65.000  0.8764    71.928  82.46\n",
      "   6   0.9449     69.440  0.8260    73.890  99.15\n",
      "   7   1.0008     66.840  0.7765    75.452  115.38\n",
      "   8   1.1208     63.260  0.7423    76.784  131.63\n",
      "   9   1.0256     65.460  0.7249    77.354  148.06\n",
      "  10   0.8897     70.950  0.6949    78.610  164.73\n",
      "  11   0.8987     71.620  0.6634    79.506  181.25\n",
      "  12   0.7129     76.400  0.6308    80.556  198.19\n",
      "  13   0.8879     70.800  0.6067    81.412  214.68\n",
      "  14   1.1286     63.390  0.5826    82.224  230.99\n",
      "  15   0.7398     76.710  0.5520    83.464  247.39\n",
      "  16   0.6997     77.480  0.5266    84.258  263.72\n",
      "  17   0.6586     79.170  0.5053    84.906  279.85\n",
      "  18   1.0177     66.340  0.4818    85.794  295.88\n",
      "  19   0.6384     79.090  0.4624    86.328  312.00\n",
      "  20   1.4089     60.360  0.4445    86.876  328.18\n",
      "  21   1.0358     66.390  0.4182    87.886  344.15\n",
      "  22   0.7292     76.890  0.3988    88.566  360.69\n",
      "  23   0.6355     79.980  0.3669    89.660  376.83\n",
      "  24   0.6350     80.350  0.3564    89.806  393.17\n",
      "  25   0.7355     77.630  0.3256    90.986  409.91\n",
      "  26   1.0561     68.070  0.3090    91.506  426.49\n",
      "  27   0.6469     79.450  0.2900    92.048  442.64\n",
      "  28   0.8654     72.440  0.2720    92.768  458.99\n",
      "  29   0.7571     77.710  0.2497    93.478  475.21\n",
      "  30   1.3964     61.530  0.2342    93.906  491.87\n",
      "  31   1.0965     66.930  0.2107    94.816  507.89\n",
      "  32   1.8104     52.620  0.1963    95.242  524.38\n",
      "  33   0.6403     81.690  0.1806    95.808  540.65\n",
      "  34   0.6261     80.930  0.1707    96.148  556.70\n",
      "  35   0.4648     86.260  0.1484    97.002  573.23\n",
      "  36   0.3976     88.100  0.1348    97.428  589.30\n",
      "  37   0.4976     84.770  0.1233    97.752  605.44\n",
      "  38   1.0196     71.200  0.1146    98.082  621.55\n",
      "  39   0.7292     79.620  0.1027    98.440  637.51\n",
      "  40   0.3018     91.740  0.0934    98.786  653.79\n",
      "  41   0.3076     91.550  0.0877    98.990  669.79\n",
      "  42   0.5069     85.930  0.0855    99.052  686.59\n",
      "  43   0.2801     92.480  0.0799    99.180  702.48\n",
      "  44   0.2852     92.240  0.0756    99.370  718.42\n",
      "  45   0.3006     91.850  0.0749    99.396  735.54\n",
      "  46   0.2770     92.600  0.0723    99.484  752.06\n",
      "  47   0.2675     92.870  0.0708    99.494  768.37\n",
      "  48   0.2654     92.970  0.0702    99.536  784.68\n",
      "  49   0.2748     92.670  0.0698    99.530  801.08\n",
      "  50   0.2654     92.910  0.0684    99.586  817.09\n",
      "Doing seed 107\n",
      "\n",
      "Use GPU: 0 for training\n",
      "==> Running with ['/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py', '-f', '/root/.local/share/jupyter/runtime/kernel-84a8d6fb-b2d8-40be-b67f-a96c7eebbe81.json']\n",
      "==> Building model..\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Memory peak: 4593393152 Bytes\n",
      "Epoch  testloss  testacc  trainloss  trainacc  time\n",
      "   1   1.9626     28.290  1.6615    37.242  16.34\n",
      "   2   1.3456     54.040  1.1351    60.256  32.37\n",
      "   3   1.3000     58.850  0.9910    66.834  48.35\n",
      "   4   1.1043     62.420  0.9251    69.784  64.76\n",
      "   5   0.9588     68.640  0.8710    72.334  81.00\n",
      "   6   1.2192     62.160  0.8259    73.670  96.97\n",
      "   7   1.1496     62.780  0.7769    75.324  113.66\n",
      "   8   1.5033     53.960  0.7466    76.560  129.65\n",
      "   9   1.0068     67.420  0.7288    77.432  146.09\n",
      "  10   0.9179     70.480  0.6951    78.404  162.06\n",
      "  11   0.7408     77.030  0.6604    79.574  178.23\n",
      "  12   1.0506     66.980  0.6385    80.374  194.38\n",
      "  13   1.0496     67.460  0.6118    81.330  210.64\n",
      "  14   0.7689     74.850  0.5774    82.562  227.11\n",
      "  15   1.0831     65.950  0.5585    83.148  243.90\n",
      "  16   1.4989     58.870  0.5374    83.894  260.14\n",
      "  17   0.7603     76.790  0.5090    84.684  276.11\n",
      "  18   0.9857     67.970  0.4807    85.794  291.99\n",
      "  19   0.6891     77.780  0.4628    86.404  308.12\n",
      "  20   1.1673     64.480  0.4374    87.274  324.17\n",
      "  21   0.6736     78.470  0.4202    87.788  340.38\n",
      "  22   1.0103     68.650  0.3939    88.714  356.38\n",
      "  23   0.7237     77.330  0.3781    89.224  372.25\n",
      "  24   1.1341     66.610  0.3468    90.226  388.14\n",
      "  25   0.9065     72.030  0.3338    90.732  404.05\n",
      "  26   0.5768     83.350  0.3103    91.426  420.12\n",
      "  27   0.5744     82.130  0.2974    91.882  436.32\n",
      "  28   0.6505     79.560  0.2718    92.694  452.63\n",
      "  29   0.7410     77.060  0.2550    93.278  468.73\n",
      "  30   0.5792     81.840  0.2342    94.066  484.59\n",
      "  31   0.6339     80.440  0.2137    94.762  500.77\n",
      "  32   0.8563     74.580  0.2004    95.142  516.91\n",
      "  33   0.6850     78.830  0.1789    95.784  533.10\n",
      "  34   1.2203     65.160  0.1642    96.422  549.34\n",
      "  35   0.8673     74.230  0.1498    96.904  566.12\n",
      "  36   0.5806     82.450  0.1353    97.346  582.21\n",
      "  37   0.5596     84.440  0.1257    97.686  598.22\n",
      "  38   0.3467     90.050  0.1131    98.104  614.37\n",
      "  39   0.3771     89.620  0.1023    98.450  630.85\n",
      "  40   0.5623     84.190  0.0974    98.650  646.88\n",
      "  41   0.3131     91.480  0.0897    98.924  662.93\n",
      "  42   0.2922     92.010  0.0837    99.088  678.76\n",
      "  43   0.2833     92.230  0.0796    99.208  694.84\n",
      "  44   0.2725     92.460  0.0770    99.296  710.95\n",
      "  45   0.2679     92.770  0.0752    99.402  727.08\n",
      "  46   0.2612     93.070  0.0732    99.424  743.14\n",
      "  47   0.2657     92.780  0.0701    99.556  759.69\n",
      "  48   0.2611     92.840  0.0701    99.522  775.92\n",
      "  49   0.2557     93.090  0.0693    99.536  792.46\n",
      "  50   0.2595     93.000  0.0702    99.528  808.62\n",
      "Doing seed 108\n",
      "\n",
      "Use GPU: 0 for training\n",
      "==> Running with ['/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py', '-f', '/root/.local/share/jupyter/runtime/kernel-84a8d6fb-b2d8-40be-b67f-a96c7eebbe81.json']\n",
      "==> Building model..\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Memory peak: 4593393152 Bytes\n",
      "Epoch  testloss  testacc  trainloss  trainacc  time\n",
      "   1   1.6678     44.530  1.6514    37.000  16.23\n",
      "   2   1.3832     53.630  1.1201    60.756  32.44\n",
      "   3   1.1312     60.840  0.9836    67.186  48.61\n",
      "   4   1.2461     58.920  0.9263    69.892  64.71\n",
      "   5   0.9856     67.850  0.8669    72.482  81.72\n",
      "   6   1.0420     65.840  0.8192    74.156  97.99\n",
      "   7   1.2467     59.450  0.7810    75.414  114.82\n",
      "   8   0.8642     71.870  0.7522    76.624  131.12\n",
      "   9   1.2530     58.780  0.7153    77.752  147.56\n",
      "  10   1.1955     61.530  0.6866    78.716  164.18\n",
      "  11   1.1156     63.690  0.6598    79.752  181.39\n",
      "  12   0.9729     67.950  0.6324    80.502  198.27\n",
      "  13   0.7618     76.330  0.6094    81.414  214.77\n",
      "  14   0.8098     73.280  0.5790    82.286  230.73\n",
      "  15   0.9288     70.790  0.5610    83.014  246.72\n",
      "  16   1.0440     66.020  0.5313    84.040  263.29\n",
      "  17   0.9019     71.230  0.5101    84.842  280.10\n",
      "  18   0.7125     77.880  0.4875    85.656  296.29\n",
      "  19   0.7373     77.910  0.4644    86.314  312.62\n",
      "  20   0.6644     78.040  0.4433    86.972  328.84\n",
      "  21   1.0715     64.720  0.4231    87.710  345.22\n",
      "  22   0.9933     68.890  0.3987    88.394  361.54\n",
      "  23   1.0534     67.030  0.3727    89.238  378.19\n",
      "  24   1.0018     69.130  0.3595    89.790  394.58\n",
      "  25   0.7136     78.290  0.3347    90.574  411.59\n",
      "  26   1.0021     70.500  0.3146    91.422  427.93\n",
      "  27   0.9821     70.390  0.2948    91.986  444.83\n",
      "  28   0.6666     79.190  0.2755    92.494  461.07\n",
      "  29   0.7642     77.000  0.2562    93.238  477.21\n",
      "  30   0.7140     78.700  0.2362    93.928  493.27\n",
      "  31   0.5491     82.990  0.2145    94.644  509.26\n",
      "  32   0.3935     88.170  0.1949    95.278  525.35\n",
      "  33   0.7279     78.850  0.1816    95.848  541.57\n",
      "  34   0.7518     78.480  0.1708    96.146  557.83\n",
      "  35   0.6917     79.560  0.1515    96.828  574.66\n",
      "  36   0.4583     86.220  0.1378    97.288  590.76\n",
      "  37   0.5030     84.770  0.1246    97.674  607.00\n",
      "  38   0.9542     74.410  0.1182    97.922  623.26\n",
      "  39   0.4256     87.860  0.1051    98.358  639.82\n",
      "  40   0.3740     89.730  0.0981    98.608  655.96\n",
      "  41   0.3775     89.880  0.0919    98.822  672.34\n",
      "  42   0.3040     91.710  0.0857    98.984  688.50\n",
      "  43   0.2963     91.990  0.0832    99.104  705.06\n",
      "  44   0.2711     92.740  0.0791    99.280  721.75\n",
      "  45   0.2983     91.900  0.0767    99.342  738.81\n",
      "  46   0.2729     92.700  0.0745    99.416  755.19\n",
      "  47   0.2708     92.710  0.0718    99.486  771.97\n",
      "  48   0.2713     92.780  0.0718    99.480  788.10\n",
      "  49   0.2712     92.700  0.0706    99.522  804.24\n",
      "  50   0.2708     92.540  0.0705    99.562  820.35\n",
      "Doing seed 109\n",
      "\n",
      "Use GPU: 0 for training\n",
      "==> Running with ['/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py', '-f', '/root/.local/share/jupyter/runtime/kernel-84a8d6fb-b2d8-40be-b67f-a96c7eebbe81.json']\n",
      "==> Building model..\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Memory peak: 4593393152 Bytes\n",
      "Epoch  testloss  testacc  trainloss  trainacc  time\n",
      "   1   1.7546     37.250  1.6262    38.522  16.23\n",
      "   2   1.4192     49.370  1.1094    61.318  32.40\n",
      "   3   1.2107     60.750  0.9772    67.498  48.62\n",
      "   4   1.4339     52.420  0.9199    69.908  64.79\n",
      "   5   1.5761     48.550  0.8722    71.936  81.11\n",
      "   6   1.1021     64.290  0.8251    73.666  97.52\n",
      "   7   1.0266     66.670  0.7808    75.410  113.97\n",
      "   8   1.0595     65.160  0.7589    76.104  130.36\n",
      "   9   0.8590     72.830  0.7216    77.274  146.72\n",
      "  10   0.8920     71.700  0.6904    78.374  163.24\n",
      "  11   0.8507     73.010  0.6604    79.434  179.17\n",
      "  12   0.7339     76.950  0.6302    80.618  195.17\n",
      "  13   0.8330     72.940  0.6103    81.254  211.45\n",
      "  14   1.2774     61.090  0.5850    82.300  228.73\n",
      "  15   0.8324     73.810  0.5542    83.344  245.75\n",
      "  16   0.7728     74.800  0.5293    84.098  262.00\n",
      "  17   0.6510     79.540  0.5072    84.876  278.25\n",
      "  18   0.8457     73.480  0.4864    85.634  294.67\n",
      "  19   0.8806     70.030  0.4606    86.576  311.18\n",
      "  20   0.6535     79.260  0.4398    86.938  327.67\n",
      "  21   0.6735     78.910  0.4177    87.876  344.59\n",
      "  22   0.8268     73.650  0.3912    88.762  360.93\n",
      "  23   0.7016     78.210  0.3777    89.172  377.58\n",
      "  24   0.6235     80.590  0.3539    89.920  394.35\n",
      "  25   0.6528     79.720  0.3305    90.744  411.24\n",
      "  26   0.9560     68.040  0.3110    91.370  428.57\n",
      "  27   0.9345     72.090  0.2907    92.136  445.52\n",
      "  28   1.2497     61.300  0.2690    92.840  462.52\n",
      "  29   1.7998     51.900  0.2523    93.284  479.20\n",
      "  30   1.8019     50.760  0.2313    94.048  496.08\n",
      "  31   0.8042     76.050  0.2163    94.500  512.89\n",
      "  32   0.5974     81.790  0.1920    95.406  529.54\n",
      "  33   1.0903     67.180  0.1760    95.952  546.32\n",
      "  34   0.5347     84.260  0.1606    96.560  563.67\n",
      "  35   0.7012     80.340  0.1467    96.908  580.81\n",
      "  36   0.8248     77.440  0.1345    97.330  597.39\n",
      "  37   0.4619     86.640  0.1219    97.760  613.98\n",
      "  38   0.7222     80.240  0.1129    98.072  630.59\n",
      "  39   0.4129     88.140  0.1043    98.464  647.54\n",
      "  40   0.5163     84.730  0.0956    98.698  664.06\n",
      "  41   0.4498     87.540  0.0892    98.908  681.03\n",
      "  42   0.3491     90.440  0.0827    99.174  697.89\n",
      "  43   0.2804     92.480  0.0803    99.160  714.15\n",
      "  44   0.2874     92.300  0.0781    99.292  730.72\n",
      "  45   0.2759     92.760  0.0757    99.352  747.16\n",
      "  46   0.2740     92.630  0.0737    99.428  764.21\n",
      "  47   0.2662     92.980  0.0731    99.446  781.12\n",
      "  48   0.2777     92.640  0.0710    99.530  797.49\n",
      "  49   0.2721     92.920  0.0693    99.572  814.01\n",
      "  50   0.2666     93.040  0.0684    99.582  830.37\n",
      "Doing seed 110\n",
      "\n",
      "Use GPU: 0 for training\n",
      "==> Running with ['/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py', '-f', '/root/.local/share/jupyter/runtime/kernel-84a8d6fb-b2d8-40be-b67f-a96c7eebbe81.json']\n",
      "==> Building model..\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Memory peak: 4593393152 Bytes\n",
      "Epoch  testloss  testacc  trainloss  trainacc  time\n",
      "   1   1.8423     38.110  1.6472    38.190  16.24\n",
      "   2   1.8744     37.370  1.1225    60.628  32.69\n",
      "   3   1.0707     66.140  0.9820    66.878  49.41\n",
      "   4   1.1278     64.080  0.9241    69.976  66.45\n",
      "   5   1.2577     58.500  0.8741    71.844  83.19\n",
      "   6   1.4322     53.610  0.8216    74.136  99.65\n",
      "   7   1.0228     67.950  0.7858    75.134  115.99\n",
      "   8   0.8920     71.190  0.7654    75.794  132.33\n",
      "   9   1.0710     64.390  0.7163    77.694  148.77\n",
      "  10   1.6832     45.770  0.6926    78.632  165.88\n",
      "  11   0.7994     74.600  0.6640    79.442  182.45\n",
      "  12   0.8678     72.760  0.6429    80.176  199.03\n",
      "  13   0.7779     74.170  0.6162    81.278  215.42\n",
      "  14   0.8646     74.100  0.5849    82.248  231.69\n",
      "  15   0.9726     69.430  0.5647    82.772  248.20\n",
      "  16   0.6726     79.140  0.5360    83.810  264.44\n",
      "  17   0.7917     75.260  0.5077    84.888  280.85\n",
      "  18   0.9951     68.890  0.4919    85.392  297.00\n",
      "  19   0.6738     78.280  0.4648    86.212  313.15\n",
      "  20   0.6419     79.270  0.4474    86.810  329.27\n",
      "  21   0.5943     82.400  0.4221    87.590  345.39\n",
      "  22   0.8801     73.640  0.4061    88.116  361.72\n",
      "  23   0.6116     81.260  0.3743    89.258  377.89\n",
      "  24   1.1642     64.640  0.3583    89.798  395.24\n",
      "  25   0.8605     72.420  0.3343    90.628  411.58\n",
      "  26   0.5617     82.350  0.3132    91.388  428.04\n",
      "  27   1.4211     59.980  0.3010    91.672  444.49\n",
      "  28   1.1213     65.720  0.2713    92.694  460.99\n",
      "  29   1.2405     63.860  0.2570    93.298  477.20\n",
      "  30   0.5824     82.510  0.2327    94.034  493.96\n",
      "  31   0.9478     72.140  0.2199    94.548  510.14\n",
      "  32   0.7896     76.620  0.1999    95.212  526.35\n",
      "  33   0.8021     76.110  0.1840    95.670  542.55\n",
      "  34   0.7582     77.250  0.1626    96.410  558.73\n",
      "  35   0.8122     76.220  0.1488    96.878  575.05\n",
      "  36   0.8546     75.500  0.1358    97.294  591.35\n",
      "  37   0.4630     86.910  0.1245    97.682  607.46\n",
      "  38   0.4085     88.230  0.1128    98.110  624.04\n",
      "  39   0.3445     90.790  0.1050    98.332  640.36\n",
      "  40   0.3995     89.020  0.0977    98.646  656.82\n",
      "  41   0.3555     90.270  0.0927    98.812  673.28\n",
      "  42   0.3712     89.770  0.0881    98.946  689.58\n",
      "  43   0.2850     92.220  0.0808    99.226  705.94\n",
      "  44   0.2689     92.980  0.0785    99.254  722.19\n",
      "  45   0.2998     91.900  0.0760    99.350  738.44\n",
      "  46   0.2763     92.740  0.0748    99.396  754.78\n",
      "  47   0.2790     92.420  0.0741    99.414  771.11\n",
      "  48   0.2729     92.810  0.0718    99.502  787.69\n",
      "  49   0.2610     93.160  0.0707    99.502  804.13\n",
      "  50   0.2629     93.020  0.0694    99.578  820.47\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# SENG\n",
    "# Copyright (c) 2021 Minghan Yang, Dong Xu, Zaiwen Wen, Mengyun Chen, Pengxiang Xu\n",
    "# All rights reserved.\n",
    "#\n",
    "# This program is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU General Public License as published by\n",
    "# the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "\n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU General Public License for more details.\n",
    "\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with this program.  If not, see <https://www.gnu.org/licenses/>.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.multiprocessing as mp\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#from resnet_ex import resnet50,resnet18\n",
    "#from vgg_ex import vgg16,vgg11,vgg16_bn\n",
    "from vgg_ex_new_net import vgg16,vgg11,vgg16_bn\n",
    "from dist_utils import rank0_print\n",
    "from label_smoothing_loss import LabelSmoothingLoss\n",
    "\n",
    "from seng import SENG\n",
    "'''\n",
    "parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
    "parser.add_argument('--datadir', default='/datasets', help='Place where data are stored')\n",
    "parser.add_argument('--lr', default=0.05, type=float, help='learning rate')\n",
    "parser.add_argument('--lr-decay-epoch', default=30, type=int, help='learning rate decay at n epoches')\n",
    "parser.add_argument('--lr-decay-rate', default=0.1, type=float, help='how much learning rate decays')\n",
    "parser.add_argument('--lr-scheme', default='staircase', type=str, help='how much learning rate decays')\n",
    "parser.add_argument('--batch-size', '-b', default=256, type=int, help='batch size across all nodes')\n",
    "parser.add_argument('--epoch', default=100, type=int, help='epoch')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, help='momentum')\n",
    "parser.add_argument('--weight-decay', default=5e-4, type=float, help='weight decay')\n",
    "parser.add_argument('--damping', default=0.05, type=float, help='initial damping')\n",
    "parser.add_argument('--curvature-update-freq', default=200, type=int,\n",
    "                    help='The frequency to update inverse fisher matrix [default 50]')\n",
    "parser.add_argument('--fim-subsample', type=int, help='subsample count of GPU')\n",
    "parser.add_argument('--fim-col-sample-size', type=int, default=256, help='subsample count of col')\n",
    "parser.add_argument('--im-size-threshold', type=int, default=700000, help='only approximate over this size')\n",
    "parser.add_argument('--label-smoothing', default=0.0, type=float, help='label smoothing parameter')\n",
    "parser.add_argument('--gpu', default=None, type=int, help='GPU id to use')\n",
    "parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
    "                    help='number of data loading workers (default: 4)')\n",
    "parser.add_argument('--trainset', default='cifar10',\n",
    "                    choices=['cifar10', 'cifar100','imagenette'],\n",
    "                    help='training dataset')\n",
    "parser.add_argument('--arch', default='vgg16_bn',\n",
    "                    choices=['vgg16', 'vgg16_bn', 'resnet50', 'resnet18'],\n",
    "                    help='model architecture')\n",
    "parser.add_argument('--world-size', default=-1, type=int,\n",
    "                    help='number of nodes for distributed training')\n",
    "parser.add_argument('--rank', default=-1, type=int,\n",
    "                    help='node rank for distributed training')\n",
    "parser.add_argument('--dist-url', default='tcp://224.66.41.62:23456', type=str,\n",
    "                    help='url used to set up distributed training')\n",
    "parser.add_argument('--dist-backend', default='nccl', type=str,\n",
    "                    help='distributed backend')\n",
    "parser.add_argument('--multiprocessing-distributed', action='store_true',\n",
    "                    help='Use multi-processing distributed training to launch '\n",
    "                         'N processes per node, which has N GPUs. This is the '\n",
    "                         'fastest way to use PyTorch for either single node or '\n",
    "                         'multi node data parallel training')\n",
    "parser.add_argument('--verbose', action='store_true',\n",
    "                    help='print process when training')\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "'''\n",
    "\n",
    "#random_seed = 101\n",
    "for random_seed in [102, 103, 104, 105, 106, 107, 108, 109, 110]:\n",
    "  print('Doing seed {}... \\n'.format(random_seed))\n",
    "  torch.manual_seed(random_seed)\n",
    "\n",
    "  basic_path = '/content/gdrive/My Drive/Randomized_KFAC_project/results_SENG'\n",
    "\n",
    "  class args_class: # fudging the class to bypass keyboard input\n",
    "    def __init__(self):\n",
    "      self.im_size_threshold = 700000\n",
    "      self.fim_col_sample_size = 128\n",
    "      self.fim_subsample = False\n",
    "      self.distributed = False  \n",
    "      \n",
    "      self.label_smoothing = 0.0\n",
    "      self.world_size = -1\n",
    "      self.gpu = 0 #cuda0\n",
    "      self.multiprocessing_distributed = False\n",
    "      self.trainset = 'cifar10'\n",
    "      self.arch = 'vgg16_bn'\n",
    "      self.datadir = './data_CIFAR10'\n",
    "      self.workers = 1\n",
    "      self.verbose = False\n",
    "\n",
    "      self.epoch = 50 #number of epochs to train for\n",
    "      self.lr_scheme = 'exp'\n",
    "      self.lr = 0.05\n",
    "      self.lr_decay_rate = 6 # 0.1\n",
    "      self.lr_decay_epoch = 75 #75# 30\n",
    "      self.damping = 2.0 #1.0\n",
    "      self.weight_decay = 1e-2 #5e-04\n",
    "      self.momentum = 0.9\n",
    "      self.batch_size = 256\n",
    "      self.curvature_update_freq = 200 #200 #50 # they say 200 in apper for both kfac and seng\n",
    "\n",
    "      #added by me:\n",
    "      self.dropout_net = False\n",
    "\n",
    "  '''\n",
    "  --gpu 0 --arch vgg16_bn --trainset cifar10 --fim-col-sample-size 128 --lr 0.05 --lr-decay-epoch 75 --lr-decay-rate 6 --weight-decay 1e-2 --lr-scheme exp --damping 2  --epoch 70\n",
    "  '''\n",
    "\n",
    "  args = args_class()\n",
    "\n",
    "  def lr_schedule(epoch, lr0):\n",
    "      lr = lr0\n",
    "      if args.lr_scheme == 'staircase':\n",
    "          lr = lr * (args.lr_decay_rate**(epoch // args.lr_decay_epoch))\n",
    "      elif args.lr_scheme == 'cosine':\n",
    "      # cosine\n",
    "          epoch_tune = args.lr_decay_epoch\n",
    "          if epoch < epoch_tune:\n",
    "              lr = 0.001 + 0.5 * (lr - 0.001) * (1 + math.cos(epoch / epoch_tune * math.pi))\n",
    "          else:\n",
    "              lr = 0.0005\n",
    "      else:\n",
    "          lr = lr * (1.0 - epoch/args.lr_decay_epoch)**args.lr_decay_rate\n",
    "      return lr\n",
    "\n",
    "  def adjust_learning_rate(optimizer, epoch, args):\n",
    "      lr = lr_schedule(epoch, args.lr)\n",
    "      for param_group in optimizer.param_groups:\n",
    "          param_group['lr'] = lr\n",
    "\n",
    "  def adjust_damping(preconditioner, epoch, args):\n",
    "      damping = args.damping * (args.lr_decay_rate**((epoch // args.lr_decay_epoch) / 5))\n",
    "      preconditioner.damping = damping\n",
    "\n",
    "\n",
    "  def main():\n",
    "      args.distributed = args.world_size > 1 or args.multiprocessing_distributed\n",
    "      ngpus_per_node = torch.cuda.device_count()\n",
    "\n",
    "      if args.multiprocessing_distributed:\n",
    "          # Since we have ngpus_per_node processes per node, the total world_size\n",
    "          # needs to be adjusted accordingly\n",
    "          args.world_size = ngpus_per_node * args.world_size\n",
    "          # Use torch.multiprocessing.spawn to launch distributed processes: the\n",
    "          # main_worker process function\n",
    "          mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))\n",
    "      else:\n",
    "          # Simply call main_worker function\n",
    "          main_worker(args.gpu, ngpus_per_node, args)\n",
    "\n",
    "  def main_worker(gpu, ngpus_per_node, args):\n",
    "      args.gpu = gpu\n",
    "\n",
    "      if args.gpu is not None:\n",
    "          print(\"Use GPU: {} for training\".format(args.gpu))\n",
    "      if args.distributed:\n",
    "          if args.multiprocessing_distributed:\n",
    "              # For multiprocessing distributed training, rank needs to be the\n",
    "              # global rank among all the processes\n",
    "              args.rank = args.rank * ngpus_per_node + gpu\n",
    "          dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
    "                                  world_size=args.world_size, rank=args.rank)\n",
    "\n",
    "\n",
    "\n",
    "      rank0_print(\"==> Running with {0}\".format(sys.argv))\n",
    "\n",
    "      cudnn.benchmark = True\n",
    "\n",
    "      rank0_print('==> Building model..')\n",
    "\n",
    "      if args.trainset == 'cifar10' or args.trainset=='imagenette':\n",
    "          datamean = (0.4914, 0.4822, 0.4465)\n",
    "          datastd = (0.2470, 0.2435, 0.2616)\n",
    "          num_classes = 10\n",
    "      elif args.trainset == 'cifar100':\n",
    "          datamean = (0.5071, 0.4867, 0.4408)\n",
    "          datastd = (0.2675, 0.2565, 0.2761)\n",
    "          num_classes = 100\n",
    "      else:\n",
    "        num_classes = 10\n",
    "\n",
    "      if args.arch == 'resnet50':\n",
    "          net = resnet50(num_classes=num_classes)\n",
    "      elif args.arch == 'resnet18':\n",
    "          net = resnet18(num_classes=num_classes)\n",
    "      elif args.arch == 'vgg16_bn':\n",
    "          net = vgg16_bn(num_classes=num_classes)\n",
    "      elif args.arch == 'vgg16':\n",
    "          net = vgg16(num_classes=num_classes)\n",
    "          \n",
    "      if args.dropout_net == True: # try to add dropout\n",
    "        feats_list = list(net.features)\n",
    "        new_feats_list = []\n",
    "        for feat in feats_list:\n",
    "            new_feats_list.append(feat)\n",
    "            if isinstance(feat, nn.Conv2d):\n",
    "                new_feats_list.append(nn.Dropout(p = 0.07, inplace = True))\n",
    "        # modify convolution layers\n",
    "        net.features = nn.Sequential(*new_feats_list)\n",
    "\n",
    "\n",
    "      if args.distributed:\n",
    "          # For multiprocessing distributed, DistributedDataParallel constructor\n",
    "          # should always set the single device scope, otherwise,\n",
    "          # DistributedDataParallel will use all available devices.\n",
    "          if args.gpu is not None:\n",
    "              torch.cuda.set_device(args.gpu)\n",
    "              net.cuda(args.gpu)\n",
    "              # When using a single GPU per process and per\n",
    "              # DistributedDataParallel, we need to divide the batch size\n",
    "              # ourselves based on the total number of GPUs we have\n",
    "              args.batch_size = int(args.batch_size / args.world_size)\n",
    "              args.workers = int((args.workers + ngpus_per_node - 1) / ngpus_per_node)\n",
    "              net = torch.nn.parallel.DistributedDataParallel(net, device_ids=[args.gpu])\n",
    "          else:\n",
    "              net.cuda()\n",
    "              # DistributedDataParallel will divide and allocate batch_size to all\n",
    "              # available GPUs if device_ids are not set\n",
    "              net = torch.nn.parallel.DistributedDataParallel(net)\n",
    "      elif args.gpu is not None:\n",
    "          torch.cuda.set_device(args.gpu)\n",
    "          net = net.cuda(args.gpu)\n",
    "\n",
    "      rank0_print('==> Preparing data..')\n",
    "\n",
    "      if args.trainset == 'imagenette' :\n",
    "          traindir = os.path.join(args.datadir, 'train')\n",
    "          valdir = os.path.join(args.datadir, 'val')\n",
    "          normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                      std=[0.229, 0.224, 0.225])\n",
    "          trainset = datasets.ImageFolder(traindir,transforms.Compose([\n",
    "              transforms.RandomResizedCrop(224),\n",
    "              transforms.RandomHorizontalFlip(),\n",
    "              transforms.ToTensor(),\n",
    "              normalize,\n",
    "          ]))\n",
    "          testset = datasets.ImageFolder(valdir, transforms.Compose([\n",
    "              transforms.Resize(256),\n",
    "              transforms.CenterCrop(224),\n",
    "              transforms.ToTensor(),\n",
    "              normalize,\n",
    "          ]))\n",
    "      else:\n",
    "          transform_train = transforms.Compose([\n",
    "              transforms.RandomCrop(32, padding=4),\n",
    "              transforms.RandomHorizontalFlip(),\n",
    "              transforms.ToTensor(),\n",
    "              transforms.Normalize(datamean, datastd),\n",
    "          ])\n",
    "\n",
    "          transform_test = transforms.Compose([\n",
    "              transforms.ToTensor(),\n",
    "              transforms.Normalize(datamean, datastd),\n",
    "          ])\n",
    "\n",
    "      if args.trainset == 'cifar10':\n",
    "          trainset = torchvision.datasets.CIFAR10(root=args.datadir, train=True, download=True, transform=transform_train)\n",
    "          testset = torchvision.datasets.CIFAR10(root=args.datadir, train=False, download=True, transform=transform_test)\n",
    "      elif args.trainset == 'cifar100':\n",
    "          trainset = torchvision.datasets.CIFAR100(root=args.datadir, train=True, download=False, transform=transform_train)\n",
    "          testset = torchvision.datasets.CIFAR100(root=args.datadir, train=False, download=False, transform=transform_test)\n",
    "\n",
    "\n",
    "      if args.distributed:\n",
    "          train_sampler = torch.utils.data.distributed.DistributedSampler(trainset)\n",
    "      else:\n",
    "          train_sampler = None\n",
    "\n",
    "      trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size, shuffle=(train_sampler is None), num_workers=args.workers, pin_memory=True, sampler=train_sampler)\n",
    "      testloader = torch.utils.data.DataLoader(testset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "      if args.label_smoothing > 0:\n",
    "          criterion = LabelSmoothingLoss(args.label_smoothing).cuda(args.gpu)\n",
    "      else:\n",
    "          criterion = nn.CrossEntropyLoss().cuda(args.gpu)\n",
    "      optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "      preconditioner = SENG(net, args.damping, update_freq=args.curvature_update_freq, verbose=args.verbose, subsample=args.fim_subsample, im_size_threshold=args.im_size_threshold, col_sample_size=args.fim_col_sample_size)\n",
    "\n",
    "      pending_batch = None\n",
    "\n",
    "      # Training\n",
    "      def train(epoch):\n",
    "          net.train()\n",
    "          train_loss = 0\n",
    "          correct = 0\n",
    "          total = 0\n",
    "          epoch_start_time = time.time()\n",
    "\n",
    "          for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "              num_iter = preconditioner.iteration_counter\n",
    "              epoch_for_adjust = epoch + (batch_idx + 1) / len(trainloader)\n",
    "              adjust_learning_rate(optimizer, epoch_for_adjust, args)\n",
    "              adjust_damping(preconditioner, epoch_for_adjust, args)\n",
    "              inputs = inputs.cuda(args.gpu, non_blocking=True)\n",
    "              targets = targets.cuda(args.gpu, non_blocking=True)\n",
    "\n",
    "              outputs = net(inputs)\n",
    "\n",
    "              loss = criterion(outputs, targets)\n",
    "              optimizer.zero_grad()\n",
    "\n",
    "              loss.backward()\n",
    "\n",
    "              preconditioner.step()\n",
    "              optimizer.step()\n",
    "\n",
    "              train_loss += loss.item()\n",
    "              _, predicted = outputs.max(1)\n",
    "              total += targets.size(0)\n",
    "              correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "              this_batch_time = time.time() - epoch_start_time\n",
    "\n",
    "              if args.verbose:\n",
    "                  if num_iter % 50 == 0:\n",
    "                      rank0_print('%3d-%4d   %2.1e  %2.1e  %2.1e  %2.1e   %2.1e  %3.1f%%' %\n",
    "                      (epoch, num_iter, loss.item(), preconditioner.state['normg'], preconditioner.state['normd'],  preconditioner.state['adg'], preconditioner.damping, correct / total * 100))\n",
    "          return train_loss / len(trainloader), correct / total\n",
    "\n",
    "\n",
    "\n",
    "      def validate(epoch):\n",
    "          # global best_acc\n",
    "          net.eval()\n",
    "          test_loss = 0\n",
    "          correct = 0\n",
    "          total = 0\n",
    "          with torch.no_grad():\n",
    "              for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "                  if args.gpu is not None:\n",
    "                      inputs = inputs.cuda(args.gpu, non_blocking=True)\n",
    "                  targets = targets.cuda(args.gpu, non_blocking=True)\n",
    "                  outputs = net(inputs)\n",
    "                  loss = criterion(outputs, targets)\n",
    "\n",
    "                  test_loss += loss.item()\n",
    "                  _, predicted = outputs.max(1)\n",
    "                  total += targets.size(0)\n",
    "                  correct += predicted.eq(targets).sum().item()\n",
    "          return test_loss / len(testloader), correct / total\n",
    "\n",
    "      total_time = 0\n",
    "\n",
    "      test_loss_list = []\n",
    "      test_acc_list = []\n",
    "      train_loss_list = []\n",
    "      train_acc_list = []\n",
    "      total_time_list = []\n",
    "\n",
    "      for epoch in range(args.epoch):\n",
    "          start_time = time.time()\n",
    "          if args.distributed:\n",
    "              train_sampler.set_epoch(epoch)\n",
    "          train_loss, train_acc = train(epoch)\n",
    "          train_time = time.time() - start_time\n",
    "          total_time += train_time\n",
    "          test_loss, test_acc = validate(epoch)\n",
    "\n",
    "          if epoch == 0:\n",
    "              mstats = torch.cuda.memory_stats()\n",
    "              rank0_print('Memory peak: %d Bytes' % mstats['active_bytes.all.peak'])\n",
    "              rank0_print('Epoch  testloss  testacc  trainloss  trainacc  time')\n",
    "\n",
    "\n",
    "    \n",
    "          rank0_print(\"  %2d   %6.4f     %6.3f  %6.4f    %6.3f  %.2f\" % (\n",
    "              epoch + 1, test_loss,\n",
    "              test_acc * 100, train_loss,\n",
    "              train_acc * 100, total_time), flush=True)\n",
    "          \n",
    "          test_loss_list.append(test_loss)\n",
    "          test_acc_list.append(test_acc)\n",
    "          train_loss_list.append(train_loss)\n",
    "          train_acc_list.append(train_acc)\n",
    "          total_time_list.append(total_time)\n",
    "          \n",
    "          ###### save data ###########\n",
    "          np.save(basic_path + '/SENG_test_loss_run{}.npy'.format(random_seed), test_loss_list)\n",
    "          np.save(basic_path + '/SENG_test_acc_run{}.npy'.format(random_seed), test_acc_list)\n",
    "          np.save(basic_path + '/SENG_train_loss_run{}.npy'.format(random_seed), train_loss_list)\n",
    "          np.save(basic_path + '/SENG_train_acc_run{}.npy'.format(random_seed), train_acc_list)\n",
    "          np.save(basic_path + '/SENG_total_time_run{}.npy'.format(random_seed), total_time_list)\n",
    "\n",
    "  #if __name__ == '__main__':\n",
    "  main()\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1654258271160,
     "user": {
      "displayName": "Constantin Puiu",
      "userId": "15538331899008717371"
     },
     "user_tz": -60
    },
    "id": "2L0sci5tCv1J"
   },
   "outputs": [],
   "source": [
    "# ERROR DEBUGING\n",
    "#import numpy as np\n",
    "#A = np.load(error_write_path + '/m_aa_when_err.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1654258271161,
     "user": {
      "displayName": "Constantin Puiu",
      "userId": "15538331899008717371"
     },
     "user_tz": -60
    },
    "id": "lp3QWG-kIHrO"
   },
   "outputs": [],
   "source": [
    "#np.linalg.matrix_rank(A)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNa9HhUo8on1lxkwgPN7jSt",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "FULLY_CONNECTED_MANUAL_SENG_MAIN_new_net.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
