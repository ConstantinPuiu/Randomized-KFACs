{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15365,
     "status": "ok",
     "timestamp": 1667835967239,
     "user": {
      "displayName": "Constantin Puiu",
      "userId": "15538331899008717371"
     },
     "user_tz": 0
    },
    "id": "fpp4PfYe-5Ig",
    "outputId": "75309dda-3cd3-41d7-b7ea-23c932a11399"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "import sys\n",
    "sys.path.append('/content/gdrive/My Drive/Colab Notebooks/Lib_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1667835967241,
     "user": {
      "displayName": "Constantin Puiu",
      "userId": "15538331899008717371"
     },
     "user_tz": 0
    },
    "id": "ckRKD0vrIGFh",
    "outputId": "79c2c7b5-c733-488e-9d2d-0115a7b5f524"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'!pip install -Uqq ipdb\\nimport ipdb\\n%pdb on'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' DEBUG'''\n",
    "'''!pip install -Uqq ipdb\n",
    "import ipdb\n",
    "%pdb on'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 413,
     "status": "ok",
     "timestamp": 1667835967636,
     "user": {
      "displayName": "Constantin Puiu",
      "userId": "15538331899008717371"
     },
     "user_tz": 0
    },
    "id": "U7PvGkQxz6I_",
    "outputId": "99eea04c-d77b-49ad-ac6b-bb6c6d577392"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov  7 15:46:06 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   27C    P0    42W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# GPU bit!\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2456,
     "status": "ok",
     "timestamp": 1667835970090,
     "user": {
      "displayName": "Constantin Puiu",
      "userId": "15538331899008717371"
     },
     "user_tz": 0
    },
    "id": "4xdyCPdJFrgK",
    "outputId": "e131dd15-ba9b-49d4-bf4d-4815b317dea1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "#GPU 2\n",
    "import torch\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "cuda0 = torch.device('cuda:0')#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1667835970092,
     "user": {
      "displayName": "Constantin Puiu",
      "userId": "15538331899008717371"
     },
     "user_tz": 0
    },
    "id": "R4LfkVcE2fb1",
    "outputId": "bf4db709-a8cd-4d49-bb3c-70dc26c74a03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your runtime has 89.6 gigabytes of available RAM\n",
      "\n",
      "You are using a high-RAM runtime!\n"
     ]
    }
   ],
   "source": [
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1824,
     "status": "ok",
     "timestamp": 1667835971903,
     "user": {
      "displayName": "Constantin Puiu",
      "userId": "15538331899008717371"
     },
     "user_tz": 0
    },
    "id": "H23_EBfIpq8d"
   },
   "outputs": [],
   "source": [
    "\"### CIFAR10 FRAMEWORK K-FAC solver\"\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from data_utils import get_dataloader\n",
    "from network_utils_for_vgg import get_network\n",
    "\n",
    "from SRE_KFAC_optimizer_file import SRE_KFACOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "556071ce7018440794a344f9f6473b65",
      "7cca248bc93a4e5c93c4618cf081ebd7",
      "1ce9f4cbce2b4ea390b166518e35bbb6",
      "2173101d247c45b7af7041ea918463c7",
      "cd913ed07cd64bc5941c77153062e141",
      "3401289cb1bf448a83bf556158a659e5",
      "930a8d6c30a147f0b1e2bf918bc049c7",
      "1adbeb91657d47e597da5da3a628755b",
      "c4501d3693e7402eacb8b7152e8ec503",
      "1f55550f09cd4807975c968d48fab58f",
      "0fb3db7e606848eeafe5aa98acd5b1a9"
     ]
    },
    "id": "APPMASzVCvNU",
    "outputId": "37ac4b1e-cb1c-4b94-80d6-d221e06662b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing random seed 131... \n",
      "\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data_CIFAR10/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556071ce7018440794a344f9f6473b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data_CIFAR10/cifar-10-python.tar.gz to ./data_CIFAR10\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of parameters is 14990922\n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (32): ReLU(inplace=True)\n",
      "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (36): ReLU(inplace=True)\n",
      "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (39): ReLU(inplace=True)\n",
      "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (42): ReLU(inplace=True)\n",
      "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "=> We keep following layers in KFAC. \n",
      "(0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(7): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(8): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(9): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(11): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(13): Linear(in_features=512, out_features=512, bias=True)\n",
      "(14): Linear(in_features=512, out_features=10, bias=True)\n",
      "Started test before training\n",
      "\n",
      "Test set: Avg. loss: 2.3022, Accuracy: 1000/10000 (10%)\n",
      "\n",
      "Finished test before training in 9.477417230606079s\n",
      "\n",
      " m_aa just after testing (should be empty if we did nto accumulate stats during traning - this si correct) {} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/content/gdrive/My Drive/Colab Notebooks/Lib_files/kfac_srevd_for_vgg16_bn_improved_inversion_FC_project_adaptive_damping_unentangled_weight_decay.py:268: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1174.)\n",
      "  p.data.add_(-group['lr'], d_p)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.364259\n",
      "param norm: 522.8438110351562\n",
      "\n",
      "Test set: Avg. loss: 1.8865, Accuracy: 3119/10000 (31%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 196, 1, 1, 1, 1, 1, 196, 1\n",
      "Lengths of input lists are 196, 1, 1, 1, 2, 1, 196, 2\n",
      "Total time: 29.82528281211853s\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.682477\n",
      "param norm: 561.2550048828125\n",
      "\n",
      "Test set: Avg. loss: 1.2202, Accuracy: 5605/10000 (56%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 392, 2, 2, 2, 2, 2, 392, 2\n",
      "Lengths of input lists are 392, 2, 2, 2, 3, 2, 392, 3\n",
      "Total time: 56.63996863365173s\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.251554\n",
      "param norm: 563.90087890625\n",
      "\n",
      "Test set: Avg. loss: 1.1381, Accuracy: 6149/10000 (61%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 588, 3, 3, 3, 3, 3, 588, 3\n",
      "Lengths of input lists are 588, 3, 3, 3, 4, 3, 588, 4\n",
      "Total time: 83.5020821094513s\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.018688\n",
      "param norm: 576.2335205078125\n",
      "\n",
      "Test set: Avg. loss: 1.2077, Accuracy: 6073/10000 (61%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 784, 4, 4, 4, 4, 4, 784, 4\n",
      "Lengths of input lists are 784, 4, 4, 4, 5, 4, 784, 5\n",
      "Total time: 110.43433666229248s\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 1.083067\n",
      "param norm: 613.962158203125\n",
      "\n",
      "Test set: Avg. loss: 1.1293, Accuracy: 6714/10000 (67%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 980, 5, 5, 5, 5, 5, 980, 5\n",
      "Lengths of input lists are 980, 5, 5, 5, 6, 5, 980, 6\n",
      "Total time: 137.23440194129944s\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.711450\n",
      "param norm: 680.3765258789062\n",
      "\n",
      "Test set: Avg. loss: 1.0697, Accuracy: 6755/10000 (68%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 1176, 6, 6, 6, 6, 6, 1176, 6\n",
      "Lengths of input lists are 1176, 6, 6, 6, 7, 6, 1176, 7\n",
      "Total time: 164.08904314041138s\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.974109\n",
      "param norm: 777.5216674804688\n",
      "\n",
      "Test set: Avg. loss: 0.8186, Accuracy: 7353/10000 (74%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 1372, 7, 7, 7, 7, 7, 1372, 7\n",
      "Lengths of input lists are 1372, 7, 7, 7, 8, 7, 1372, 8\n",
      "Total time: 190.88537549972534s\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.638574\n",
      "param norm: 908.6363525390625\n",
      "\n",
      "Test set: Avg. loss: 0.6142, Accuracy: 8076/10000 (81%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 1568, 8, 8, 8, 8, 8, 1568, 8\n",
      "Lengths of input lists are 1568, 8, 8, 8, 9, 8, 1568, 9\n",
      "Total time: 217.53621435165405s\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.523896\n",
      "param norm: 1058.4051513671875\n",
      "\n",
      "Test set: Avg. loss: 0.5537, Accuracy: 8275/10000 (83%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 1764, 9, 9, 9, 9, 9, 1764, 9\n",
      "Lengths of input lists are 1764, 9, 9, 9, 10, 9, 1764, 10\n",
      "Total time: 244.224675655365s\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.603280\n",
      "param norm: 1240.3095703125\n",
      "\n",
      "Test set: Avg. loss: 0.5856, Accuracy: 8187/10000 (82%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 1960, 10, 10, 10, 10, 10, 1960, 10\n",
      "Lengths of input lists are 1960, 10, 10, 10, 11, 10, 1960, 11\n",
      "Total time: 270.81230783462524s\n",
      "Train Epoch: 11 [0/50000 (0%)]\tLoss: 0.493766\n",
      "param norm: 1439.125732421875\n",
      "\n",
      "Test set: Avg. loss: 0.4615, Accuracy: 8488/10000 (85%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 2156, 11, 11, 11, 11, 11, 2156, 11\n",
      "Lengths of input lists are 2156, 11, 11, 11, 12, 11, 2156, 12\n",
      "Total time: 297.63616156578064s\n",
      "Train Epoch: 12 [0/50000 (0%)]\tLoss: 0.367673\n",
      "param norm: 1630.2320556640625\n",
      "\n",
      "Test set: Avg. loss: 0.4440, Accuracy: 8555/10000 (86%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 2352, 12, 12, 12, 12, 12, 2352, 12\n",
      "Lengths of input lists are 2352, 12, 12, 12, 13, 12, 2352, 13\n",
      "Total time: 324.42956495285034s\n",
      "Train Epoch: 13 [0/50000 (0%)]\tLoss: 0.321359\n",
      "param norm: 1801.72705078125\n",
      "\n",
      "Test set: Avg. loss: 0.2993, Accuracy: 9048/10000 (90%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 2548, 13, 13, 13, 13, 13, 2548, 13\n",
      "Lengths of input lists are 2548, 13, 13, 13, 14, 13, 2548, 14\n",
      "Total time: 350.65159726142883s\n",
      "Train Epoch: 14 [0/50000 (0%)]\tLoss: 0.179803\n",
      "param norm: 1815.362060546875\n",
      "\n",
      "Test set: Avg. loss: 0.3088, Accuracy: 9056/10000 (91%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 2744, 14, 14, 14, 14, 14, 2744, 14\n",
      "Lengths of input lists are 2744, 14, 14, 14, 15, 14, 2744, 15\n",
      "Total time: 377.30751848220825s\n",
      "Train Epoch: 15 [0/50000 (0%)]\tLoss: 0.225942\n",
      "param norm: 1831.4593505859375\n",
      "\n",
      "Test set: Avg. loss: 0.3140, Accuracy: 9027/10000 (90%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 2940, 15, 15, 15, 15, 15, 2940, 15\n",
      "Lengths of input lists are 2940, 15, 15, 15, 16, 15, 2940, 16\n",
      "Total time: 403.8230426311493s\n",
      "Train Epoch: 16 [0/50000 (0%)]\tLoss: 0.110585\n",
      "param norm: 1849.4593505859375\n",
      "\n",
      "Test set: Avg. loss: 0.3078, Accuracy: 9095/10000 (91%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 3136, 16, 16, 16, 16, 16, 3136, 16\n",
      "Lengths of input lists are 3136, 16, 16, 16, 17, 16, 3136, 17\n",
      "Total time: 430.3308103084564s\n",
      "Train Epoch: 17 [0/50000 (0%)]\tLoss: 0.146475\n",
      "param norm: 1869.2982177734375\n",
      "\n",
      "Test set: Avg. loss: 0.3279, Accuracy: 9062/10000 (91%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 3332, 17, 17, 17, 17, 17, 3332, 17\n",
      "Lengths of input lists are 3332, 17, 17, 17, 18, 17, 3332, 18\n",
      "Total time: 456.99373984336853s\n",
      "Train Epoch: 18 [0/50000 (0%)]\tLoss: 0.134809\n",
      "param norm: 1888.1016845703125\n",
      "\n",
      "Test set: Avg. loss: 0.2930, Accuracy: 9198/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 3528, 18, 18, 18, 18, 18, 3528, 18\n",
      "Lengths of input lists are 3528, 18, 18, 18, 19, 18, 3528, 19\n",
      "Total time: 483.53916239738464s\n",
      "Train Epoch: 19 [0/50000 (0%)]\tLoss: 0.055222\n",
      "param norm: 1890.031494140625\n",
      "\n",
      "Test set: Avg. loss: 0.2919, Accuracy: 9214/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 3724, 19, 19, 19, 19, 19, 3724, 19\n",
      "Lengths of input lists are 3724, 19, 19, 19, 20, 19, 3724, 20\n",
      "Total time: 510.1118519306183s\n",
      "Train Epoch: 20 [0/50000 (0%)]\tLoss: 0.069091\n",
      "param norm: 1892.3885498046875\n",
      "\n",
      "Test set: Avg. loss: 0.2959, Accuracy: 9207/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 3920, 20, 20, 20, 20, 20, 3920, 20\n",
      "Lengths of input lists are 3920, 20, 20, 20, 21, 20, 3920, 21\n",
      "Total time: 537.1684195995331s\n",
      "Train Epoch: 21 [0/50000 (0%)]\tLoss: 0.053784\n",
      "param norm: 1894.744384765625\n",
      "\n",
      "Test set: Avg. loss: 0.3094, Accuracy: 9211/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 4116, 21, 21, 21, 21, 21, 4116, 21\n",
      "Lengths of input lists are 4116, 21, 21, 21, 22, 21, 4116, 22\n",
      "Total time: 564.5407247543335s\n",
      "Train Epoch: 22 [0/50000 (0%)]\tLoss: 0.031922\n",
      "param norm: 1897.1402587890625\n",
      "\n",
      "Test set: Avg. loss: 0.3145, Accuracy: 9187/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 4312, 22, 22, 22, 22, 22, 4312, 22\n",
      "Lengths of input lists are 4312, 22, 22, 22, 23, 22, 4312, 23\n",
      "Total time: 591.7361197471619s\n",
      "Train Epoch: 23 [0/50000 (0%)]\tLoss: 0.104263\n",
      "param norm: 1899.3984375\n",
      "\n",
      "Test set: Avg. loss: 0.3310, Accuracy: 9171/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 4508, 23, 23, 23, 23, 23, 4508, 23\n",
      "Lengths of input lists are 4508, 23, 23, 23, 24, 23, 4508, 24\n",
      "Total time: 619.0338563919067s\n",
      "Train Epoch: 24 [0/50000 (0%)]\tLoss: 0.055841\n",
      "param norm: 1901.8448486328125\n",
      "\n",
      "Test set: Avg. loss: 0.3113, Accuracy: 9201/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 4704, 24, 24, 24, 24, 24, 4704, 24\n",
      "Lengths of input lists are 4704, 24, 24, 24, 25, 24, 4704, 25\n",
      "Total time: 646.2909543514252s\n",
      "Train Epoch: 25 [0/50000 (0%)]\tLoss: 0.057359\n",
      "param norm: 1904.486328125\n",
      "\n",
      "Test set: Avg. loss: 0.3300, Accuracy: 9184/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 4900, 25, 25, 25, 25, 25, 4900, 25\n",
      "Lengths of input lists are 4900, 25, 25, 25, 26, 25, 4900, 26\n",
      "Total time: 673.7087869644165s\n",
      "Train Epoch: 26 [0/50000 (0%)]\tLoss: 0.051316\n",
      "param norm: 1909.27880859375\n",
      "\n",
      "Test set: Avg. loss: 0.3406, Accuracy: 9213/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 5096, 26, 26, 26, 26, 26, 5096, 26\n",
      "Lengths of input lists are 5096, 26, 26, 26, 27, 26, 5096, 27\n",
      "Total time: 700.998300075531s\n",
      "Train Epoch: 27 [0/50000 (0%)]\tLoss: 0.056180\n",
      "param norm: 1912.93212890625\n",
      "\n",
      "Test set: Avg. loss: 0.3369, Accuracy: 9235/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 5292, 27, 27, 27, 27, 27, 5292, 27\n",
      "Lengths of input lists are 5292, 27, 27, 27, 28, 27, 5292, 28\n",
      "Total time: 728.3970417976379s\n",
      "Train Epoch: 28 [0/50000 (0%)]\tLoss: 0.027127\n",
      "param norm: 1913.353271484375\n",
      "\n",
      "Test set: Avg. loss: 0.3426, Accuracy: 9222/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 5488, 28, 28, 28, 28, 28, 5488, 28\n",
      "Lengths of input lists are 5488, 28, 28, 28, 29, 28, 5488, 29\n",
      "Total time: 755.373517036438s\n",
      "Train Epoch: 29 [0/50000 (0%)]\tLoss: 0.040107\n",
      "param norm: 1914.079345703125\n",
      "\n",
      "Test set: Avg. loss: 0.3520, Accuracy: 9222/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 5684, 29, 29, 29, 29, 29, 5684, 29\n",
      "Lengths of input lists are 5684, 29, 29, 29, 30, 29, 5684, 30\n",
      "Total time: 782.7765691280365s\n",
      "Train Epoch: 30 [0/50000 (0%)]\tLoss: 0.023136\n",
      "param norm: 1914.591796875\n",
      "\n",
      "Test set: Avg. loss: 0.3587, Accuracy: 9230/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 5880, 30, 30, 30, 30, 30, 5880, 30\n",
      "Lengths of input lists are 5880, 30, 30, 30, 31, 30, 5880, 31\n",
      "Total time: 809.8233561515808s\n",
      "Train Epoch: 31 [0/50000 (0%)]\tLoss: 0.038474\n",
      "param norm: 1915.086669921875\n",
      "\n",
      "Test set: Avg. loss: 0.3653, Accuracy: 9232/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 6076, 31, 31, 31, 31, 31, 6076, 31\n",
      "Lengths of input lists are 6076, 31, 31, 31, 32, 31, 6076, 32\n",
      "Total time: 837.2274639606476s\n",
      "Train Epoch: 32 [0/50000 (0%)]\tLoss: 0.030190\n",
      "param norm: 1915.6353759765625\n",
      "\n",
      "Test set: Avg. loss: 0.3718, Accuracy: 9219/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 6272, 32, 32, 32, 32, 32, 6272, 32\n",
      "Lengths of input lists are 6272, 32, 32, 32, 33, 32, 6272, 33\n",
      "Total time: 864.6259429454803s\n",
      "Train Epoch: 33 [0/50000 (0%)]\tLoss: 0.050803\n",
      "param norm: 1916.2001953125\n",
      "\n",
      "Test set: Avg. loss: 0.3756, Accuracy: 9237/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 6468, 33, 33, 33, 33, 33, 6468, 33\n",
      "Lengths of input lists are 6468, 33, 33, 33, 34, 33, 6468, 34\n",
      "Total time: 891.645350933075s\n",
      "Train Epoch: 34 [0/50000 (0%)]\tLoss: 0.015975\n",
      "param norm: 1916.651123046875\n",
      "\n",
      "Test set: Avg. loss: 0.3698, Accuracy: 9225/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 6664, 34, 34, 34, 34, 34, 6664, 34\n",
      "Lengths of input lists are 6664, 34, 34, 34, 35, 34, 6664, 35\n",
      "Total time: 919.0757505893707s\n",
      "Train Epoch: 35 [0/50000 (0%)]\tLoss: 0.030740\n",
      "param norm: 1917.144287109375\n",
      "\n",
      "Test set: Avg. loss: 0.3844, Accuracy: 9237/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 6860, 35, 35, 35, 35, 35, 6860, 35\n",
      "Lengths of input lists are 6860, 35, 35, 35, 36, 35, 6860, 36\n",
      "Total time: 946.293924331665s\n",
      "Train Epoch: 36 [0/50000 (0%)]\tLoss: 0.035957\n",
      "param norm: 1918.0078125\n",
      "\n",
      "Test set: Avg. loss: 0.4023, Accuracy: 9234/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 7056, 36, 36, 36, 36, 36, 7056, 36\n",
      "Lengths of input lists are 7056, 36, 36, 36, 37, 36, 7056, 37\n",
      "Total time: 973.661098241806s\n",
      "Train Epoch: 37 [0/50000 (0%)]\tLoss: 0.076964\n",
      "param norm: 1918.9530029296875\n",
      "\n",
      "Test set: Avg. loss: 0.4071, Accuracy: 9231/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 7252, 37, 37, 37, 37, 37, 7252, 37\n",
      "Lengths of input lists are 7252, 37, 37, 37, 38, 37, 7252, 38\n",
      "Total time: 1000.8876008987427s\n",
      "Train Epoch: 38 [0/50000 (0%)]\tLoss: 0.013537\n",
      "param norm: 1919.553955078125\n",
      "\n",
      "Test set: Avg. loss: 0.4126, Accuracy: 9211/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 7448, 38, 38, 38, 38, 38, 7448, 38\n",
      "Lengths of input lists are 7448, 38, 38, 38, 39, 38, 7448, 39\n",
      "Total time: 1028.2477824687958s\n",
      "Train Epoch: 39 [0/50000 (0%)]\tLoss: 0.015202\n",
      "param norm: 1920.28271484375\n",
      "\n",
      "Test set: Avg. loss: 0.4265, Accuracy: 9224/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 7644, 39, 39, 39, 39, 39, 7644, 39\n",
      "Lengths of input lists are 7644, 39, 39, 39, 40, 39, 7644, 40\n",
      "Total time: 1055.3120546340942s\n",
      "Train Epoch: 40 [0/50000 (0%)]\tLoss: 0.031789\n",
      "param norm: 1921.024169921875\n",
      "\n",
      "Test set: Avg. loss: 0.4279, Accuracy: 9239/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 7840, 40, 40, 40, 40, 40, 7840, 40\n",
      "Lengths of input lists are 7840, 40, 40, 40, 41, 40, 7840, 41\n",
      "Total time: 1082.6396930217743s\n",
      "Train Epoch: 41 [0/50000 (0%)]\tLoss: 0.050025\n",
      "param norm: 1921.1689453125\n",
      "\n",
      "Test set: Avg. loss: 0.4334, Accuracy: 9227/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 8036, 41, 41, 41, 41, 41, 8036, 41\n",
      "Lengths of input lists are 8036, 41, 41, 41, 42, 41, 8036, 42\n",
      "Total time: 1109.8015143871307s\n",
      "Train Epoch: 42 [0/50000 (0%)]\tLoss: 0.004061\n",
      "param norm: 1921.3109130859375\n",
      "\n",
      "Test set: Avg. loss: 0.4316, Accuracy: 9227/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 8232, 42, 42, 42, 42, 42, 8232, 42\n",
      "Lengths of input lists are 8232, 42, 42, 42, 43, 42, 8232, 43\n",
      "Total time: 1137.2965121269226s\n",
      "Train Epoch: 43 [0/50000 (0%)]\tLoss: 0.016112\n",
      "param norm: 1921.4696044921875\n",
      "\n",
      "Test set: Avg. loss: 0.4328, Accuracy: 9242/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 8428, 43, 43, 43, 43, 43, 8428, 43\n",
      "Lengths of input lists are 8428, 43, 43, 43, 44, 43, 8428, 44\n",
      "Total time: 1164.4118371009827s\n",
      "Train Epoch: 44 [0/50000 (0%)]\tLoss: 0.022088\n",
      "param norm: 1921.58984375\n",
      "\n",
      "Test set: Avg. loss: 0.4364, Accuracy: 9241/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 8624, 44, 44, 44, 44, 44, 8624, 44\n",
      "Lengths of input lists are 8624, 44, 44, 44, 45, 44, 8624, 45\n",
      "Total time: 1191.9593303203583s\n",
      "Train Epoch: 45 [0/50000 (0%)]\tLoss: 0.019525\n",
      "param norm: 1921.70556640625\n",
      "\n",
      "Test set: Avg. loss: 0.4439, Accuracy: 9230/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 8820, 45, 45, 45, 45, 45, 8820, 45\n",
      "Lengths of input lists are 8820, 45, 45, 45, 46, 45, 8820, 46\n",
      "Total time: 1219.1236743927002s\n",
      "Train Epoch: 46 [0/50000 (0%)]\tLoss: 0.013019\n",
      "param norm: 1921.9298095703125\n",
      "\n",
      "Test set: Avg. loss: 0.4431, Accuracy: 9234/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 9016, 46, 46, 46, 46, 46, 9016, 46\n",
      "Lengths of input lists are 9016, 46, 46, 46, 47, 46, 9016, 47\n",
      "Total time: 1246.641485452652s\n",
      "Train Epoch: 47 [0/50000 (0%)]\tLoss: 0.007464\n",
      "param norm: 1922.010009765625\n",
      "\n",
      "Test set: Avg. loss: 0.4453, Accuracy: 9235/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 9212, 47, 47, 47, 47, 47, 9212, 47\n",
      "Lengths of input lists are 9212, 47, 47, 47, 48, 47, 9212, 48\n",
      "Total time: 1274.2337057590485s\n",
      "Train Epoch: 48 [0/50000 (0%)]\tLoss: 0.007750\n",
      "param norm: 1922.0986328125\n",
      "\n",
      "Test set: Avg. loss: 0.4475, Accuracy: 9232/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 9408, 48, 48, 48, 48, 48, 9408, 48\n",
      "Lengths of input lists are 9408, 48, 48, 48, 49, 48, 9408, 49\n",
      "Total time: 1301.3051934242249s\n",
      "Train Epoch: 49 [0/50000 (0%)]\tLoss: 0.023297\n",
      "param norm: 1922.271728515625\n",
      "\n",
      "Test set: Avg. loss: 0.4478, Accuracy: 9243/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 9604, 49, 49, 49, 49, 49, 9604, 49\n",
      "Lengths of input lists are 9604, 49, 49, 49, 50, 49, 9604, 50\n",
      "Total time: 1328.7200305461884s\n",
      "Train Epoch: 50 [0/50000 (0%)]\tLoss: 0.024950\n",
      "param norm: 1922.4180908203125\n",
      "\n",
      "Test set: Avg. loss: 0.4581, Accuracy: 9234/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 9800, 50, 50, 50, 50, 50, 9800, 50\n",
      "Lengths of input lists are 9800, 50, 50, 50, 51, 50, 9800, 51\n",
      "Total time: 1355.7060554027557s\n",
      "Doing random seed 132... \n",
      "\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "the number of parameters is 14990922\n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (32): ReLU(inplace=True)\n",
      "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (36): ReLU(inplace=True)\n",
      "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (39): ReLU(inplace=True)\n",
      "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (42): ReLU(inplace=True)\n",
      "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "=> We keep following layers in KFAC. \n",
      "(0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(7): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(8): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(9): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(11): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(13): Linear(in_features=512, out_features=512, bias=True)\n",
      "(14): Linear(in_features=512, out_features=10, bias=True)\n",
      "Started test before training\n",
      "\n",
      "Test set: Avg. loss: 2.3027, Accuracy: 1000/10000 (10%)\n",
      "\n",
      "Finished test before training in 2.0741891860961914s\n",
      "\n",
      " m_aa just after testing (should be empty if we did nto accumulate stats during traning - this si correct) {} \n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.392978\n",
      "param norm: 522.96826171875\n",
      "\n",
      "Test set: Avg. loss: 1.7928, Accuracy: 3479/10000 (35%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 196, 1, 1, 1, 1, 1, 196, 1\n",
      "Lengths of input lists are 196, 1, 1, 1, 2, 1, 196, 2\n",
      "Total time: 26.59763526916504s\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.782455\n",
      "param norm: 559.9595336914062\n",
      "\n",
      "Test set: Avg. loss: 1.4356, Accuracy: 4982/10000 (50%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 392, 2, 2, 2, 2, 2, 392, 2\n",
      "Lengths of input lists are 392, 2, 2, 2, 3, 2, 392, 3\n",
      "Total time: 53.42866039276123s\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.293616\n",
      "param norm: 562.9594116210938\n",
      "\n",
      "Test set: Avg. loss: 1.2130, Accuracy: 6050/10000 (60%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 588, 3, 3, 3, 3, 3, 588, 3\n",
      "Lengths of input lists are 588, 3, 3, 3, 4, 3, 588, 4\n",
      "Total time: 80.19636726379395s\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.896227\n",
      "param norm: 574.7926025390625\n",
      "\n",
      "Test set: Avg. loss: 1.2381, Accuracy: 6547/10000 (65%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 784, 4, 4, 4, 4, 4, 784, 4\n",
      "Lengths of input lists are 784, 4, 4, 4, 5, 4, 784, 5\n",
      "Total time: 107.05850958824158s\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.827625\n",
      "param norm: 611.458984375\n",
      "\n",
      "Test set: Avg. loss: 1.6136, Accuracy: 5416/10000 (54%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 980, 5, 5, 5, 5, 5, 980, 5\n",
      "Lengths of input lists are 980, 5, 5, 5, 6, 5, 980, 6\n",
      "Total time: 133.94706535339355s\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 1.142664\n",
      "param norm: 675.6317749023438\n",
      "\n",
      "Test set: Avg. loss: 1.3073, Accuracy: 6561/10000 (66%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 1176, 6, 6, 6, 6, 6, 1176, 6\n",
      "Lengths of input lists are 1176, 6, 6, 6, 7, 6, 1176, 7\n",
      "Total time: 160.68546533584595s\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.827575\n",
      "param norm: 769.9505615234375\n",
      "\n",
      "Test set: Avg. loss: 0.8630, Accuracy: 7231/10000 (72%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 1372, 7, 7, 7, 7, 7, 1372, 7\n",
      "Lengths of input lists are 1372, 7, 7, 7, 8, 7, 1372, 8\n",
      "Total time: 187.37914752960205s\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.745840\n",
      "param norm: 899.16748046875\n",
      "\n",
      "Test set: Avg. loss: 0.6820, Accuracy: 7829/10000 (78%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 1568, 8, 8, 8, 8, 8, 1568, 8\n",
      "Lengths of input lists are 1568, 8, 8, 8, 9, 8, 1568, 9\n",
      "Total time: 214.00790452957153s\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.535926\n",
      "param norm: 1052.5499267578125\n",
      "\n",
      "Test set: Avg. loss: 0.6486, Accuracy: 7985/10000 (80%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 1764, 9, 9, 9, 9, 9, 1764, 9\n",
      "Lengths of input lists are 1764, 9, 9, 9, 10, 9, 1764, 10\n",
      "Total time: 240.69350337982178s\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.634591\n",
      "param norm: 1230.6044921875\n",
      "\n",
      "Test set: Avg. loss: 0.6106, Accuracy: 8087/10000 (81%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 1960, 10, 10, 10, 10, 10, 1960, 10\n",
      "Lengths of input lists are 1960, 10, 10, 10, 11, 10, 1960, 11\n",
      "Total time: 267.3768241405487s\n",
      "Train Epoch: 11 [0/50000 (0%)]\tLoss: 0.474497\n",
      "param norm: 1430.6158447265625\n",
      "\n",
      "Test set: Avg. loss: 0.5504, Accuracy: 8234/10000 (82%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 2156, 11, 11, 11, 11, 11, 2156, 11\n",
      "Lengths of input lists are 2156, 11, 11, 11, 12, 11, 2156, 12\n",
      "Total time: 294.23247241973877s\n",
      "Train Epoch: 12 [0/50000 (0%)]\tLoss: 0.553043\n",
      "param norm: 1630.892578125\n",
      "\n",
      "Test set: Avg. loss: 0.5121, Accuracy: 8430/10000 (84%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 2352, 12, 12, 12, 12, 12, 2352, 12\n",
      "Lengths of input lists are 2352, 12, 12, 12, 13, 12, 2352, 13\n",
      "Total time: 320.8787155151367s\n",
      "Train Epoch: 13 [0/50000 (0%)]\tLoss: 0.314028\n",
      "param norm: 1825.5379638671875\n",
      "\n",
      "Test set: Avg. loss: 0.3030, Accuracy: 9017/10000 (90%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 2548, 13, 13, 13, 13, 13, 2548, 13\n",
      "Lengths of input lists are 2548, 13, 13, 13, 14, 13, 2548, 14\n",
      "Total time: 347.2731795310974s\n",
      "Train Epoch: 14 [0/50000 (0%)]\tLoss: 0.138866\n",
      "param norm: 1841.61962890625\n",
      "\n",
      "Test set: Avg. loss: 0.2928, Accuracy: 9043/10000 (90%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 2744, 14, 14, 14, 14, 14, 2744, 14\n",
      "Lengths of input lists are 2744, 14, 14, 14, 15, 14, 2744, 15\n",
      "Total time: 374.0543234348297s\n",
      "Train Epoch: 15 [0/50000 (0%)]\tLoss: 0.126285\n",
      "param norm: 1859.7847900390625\n",
      "\n",
      "Test set: Avg. loss: 0.2980, Accuracy: 9051/10000 (91%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 2940, 15, 15, 15, 15, 15, 2940, 15\n",
      "Lengths of input lists are 2940, 15, 15, 15, 16, 15, 2940, 16\n",
      "Total time: 400.94669127464294s\n",
      "Train Epoch: 16 [0/50000 (0%)]\tLoss: 0.164322\n",
      "param norm: 1878.1556396484375\n",
      "\n",
      "Test set: Avg. loss: 0.2986, Accuracy: 9081/10000 (91%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 3136, 16, 16, 16, 16, 16, 3136, 16\n",
      "Lengths of input lists are 3136, 16, 16, 16, 17, 16, 3136, 17\n",
      "Total time: 427.70291924476624s\n",
      "Train Epoch: 17 [0/50000 (0%)]\tLoss: 0.125412\n",
      "param norm: 1896.2091064453125\n",
      "\n",
      "Test set: Avg. loss: 0.3194, Accuracy: 9065/10000 (91%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 3332, 17, 17, 17, 17, 17, 3332, 17\n",
      "Lengths of input lists are 3332, 17, 17, 17, 18, 17, 3332, 18\n",
      "Total time: 454.7997899055481s\n",
      "Train Epoch: 18 [0/50000 (0%)]\tLoss: 0.122376\n",
      "param norm: 1913.8499755859375\n",
      "\n",
      "Test set: Avg. loss: 0.2901, Accuracy: 9174/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 3528, 18, 18, 18, 18, 18, 3528, 18\n",
      "Lengths of input lists are 3528, 18, 18, 18, 19, 18, 3528, 19\n",
      "Total time: 481.5452227592468s\n",
      "Train Epoch: 19 [0/50000 (0%)]\tLoss: 0.063541\n",
      "param norm: 1915.8148193359375\n",
      "\n",
      "Test set: Avg. loss: 0.2962, Accuracy: 9160/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 3724, 19, 19, 19, 19, 19, 3724, 19\n",
      "Lengths of input lists are 3724, 19, 19, 19, 20, 19, 3724, 20\n",
      "Total time: 508.39180970191956s\n",
      "Train Epoch: 20 [0/50000 (0%)]\tLoss: 0.073203\n",
      "param norm: 1917.568359375\n",
      "\n",
      "Test set: Avg. loss: 0.3057, Accuracy: 9176/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 3920, 20, 20, 20, 20, 20, 3920, 20\n",
      "Lengths of input lists are 3920, 20, 20, 20, 21, 20, 3920, 21\n",
      "Total time: 535.7437562942505s\n",
      "Train Epoch: 21 [0/50000 (0%)]\tLoss: 0.079488\n",
      "param norm: 1919.37060546875\n",
      "\n",
      "Test set: Avg. loss: 0.3087, Accuracy: 9182/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 4116, 21, 21, 21, 21, 21, 4116, 21\n",
      "Lengths of input lists are 4116, 21, 21, 21, 22, 21, 4116, 22\n",
      "Total time: 563.767196893692s\n",
      "Train Epoch: 22 [0/50000 (0%)]\tLoss: 0.041079\n",
      "param norm: 1921.58642578125\n",
      "\n",
      "Test set: Avg. loss: 0.3172, Accuracy: 9187/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 4312, 22, 22, 22, 22, 22, 4312, 22\n",
      "Lengths of input lists are 4312, 22, 22, 22, 23, 22, 4312, 23\n",
      "Total time: 591.4487504959106s\n",
      "Train Epoch: 23 [0/50000 (0%)]\tLoss: 0.070822\n",
      "param norm: 1923.34130859375\n",
      "\n",
      "Test set: Avg. loss: 0.3231, Accuracy: 9188/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 4508, 23, 23, 23, 23, 23, 4508, 23\n",
      "Lengths of input lists are 4508, 23, 23, 23, 24, 23, 4508, 24\n",
      "Total time: 619.0533068180084s\n",
      "Train Epoch: 24 [0/50000 (0%)]\tLoss: 0.063141\n",
      "param norm: 1924.5732421875\n",
      "\n",
      "Test set: Avg. loss: 0.3357, Accuracy: 9176/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 4704, 24, 24, 24, 24, 24, 4704, 24\n",
      "Lengths of input lists are 4704, 24, 24, 24, 25, 24, 4704, 25\n",
      "Total time: 646.5364737510681s\n",
      "Train Epoch: 25 [0/50000 (0%)]\tLoss: 0.094398\n",
      "param norm: 1926.325439453125\n",
      "\n",
      "Test set: Avg. loss: 0.3440, Accuracy: 9186/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 4900, 25, 25, 25, 25, 25, 4900, 25\n",
      "Lengths of input lists are 4900, 25, 25, 25, 26, 25, 4900, 26\n",
      "Total time: 674.1280679702759s\n",
      "Train Epoch: 26 [0/50000 (0%)]\tLoss: 0.034232\n",
      "param norm: 1929.412109375\n",
      "\n",
      "Test set: Avg. loss: 0.3414, Accuracy: 9187/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 5096, 26, 26, 26, 26, 26, 5096, 26\n",
      "Lengths of input lists are 5096, 26, 26, 26, 27, 26, 5096, 27\n",
      "Total time: 701.3615522384644s\n",
      "Train Epoch: 27 [0/50000 (0%)]\tLoss: 0.044883\n",
      "param norm: 1932.5972900390625\n",
      "\n",
      "Test set: Avg. loss: 0.3469, Accuracy: 9202/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 5292, 27, 27, 27, 27, 27, 5292, 27\n",
      "Lengths of input lists are 5292, 27, 27, 27, 28, 27, 5292, 28\n",
      "Total time: 729.201429605484s\n",
      "Train Epoch: 28 [0/50000 (0%)]\tLoss: 0.041332\n",
      "param norm: 1932.976318359375\n",
      "\n",
      "Test set: Avg. loss: 0.3538, Accuracy: 9211/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 5488, 28, 28, 28, 28, 28, 5488, 28\n",
      "Lengths of input lists are 5488, 28, 28, 28, 29, 28, 5488, 29\n",
      "Total time: 756.504555940628s\n",
      "Train Epoch: 29 [0/50000 (0%)]\tLoss: 0.037678\n",
      "param norm: 1933.4136962890625\n",
      "\n",
      "Test set: Avg. loss: 0.3614, Accuracy: 9220/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 5684, 29, 29, 29, 29, 29, 5684, 29\n",
      "Lengths of input lists are 5684, 29, 29, 29, 30, 29, 5684, 30\n",
      "Total time: 784.1432874202728s\n",
      "Train Epoch: 30 [0/50000 (0%)]\tLoss: 0.036598\n",
      "param norm: 1933.780029296875\n",
      "\n",
      "Test set: Avg. loss: 0.3615, Accuracy: 9226/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 5880, 30, 30, 30, 30, 30, 5880, 30\n",
      "Lengths of input lists are 5880, 30, 30, 30, 31, 30, 5880, 31\n",
      "Total time: 811.555046081543s\n",
      "Train Epoch: 31 [0/50000 (0%)]\tLoss: 0.014551\n",
      "param norm: 1934.162353515625\n",
      "\n",
      "Test set: Avg. loss: 0.3651, Accuracy: 9214/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 6076, 31, 31, 31, 31, 31, 6076, 31\n",
      "Lengths of input lists are 6076, 31, 31, 31, 32, 31, 6076, 32\n",
      "Total time: 839.1134588718414s\n",
      "Train Epoch: 32 [0/50000 (0%)]\tLoss: 0.024383\n",
      "param norm: 1934.5950927734375\n",
      "\n",
      "Test set: Avg. loss: 0.3740, Accuracy: 9224/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 6272, 32, 32, 32, 32, 32, 6272, 32\n",
      "Lengths of input lists are 6272, 32, 32, 32, 33, 32, 6272, 33\n",
      "Total time: 866.8103296756744s\n",
      "Train Epoch: 33 [0/50000 (0%)]\tLoss: 0.008414\n",
      "param norm: 1934.9608154296875\n",
      "\n",
      "Test set: Avg. loss: 0.3852, Accuracy: 9230/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 6468, 33, 33, 33, 33, 33, 6468, 33\n",
      "Lengths of input lists are 6468, 33, 33, 33, 34, 33, 6468, 34\n",
      "Total time: 894.1159195899963s\n",
      "Train Epoch: 34 [0/50000 (0%)]\tLoss: 0.061960\n",
      "param norm: 1935.3114013671875\n",
      "\n",
      "Test set: Avg. loss: 0.3833, Accuracy: 9210/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 6664, 34, 34, 34, 34, 34, 6664, 34\n",
      "Lengths of input lists are 6664, 34, 34, 34, 35, 34, 6664, 35\n",
      "Total time: 921.478785276413s\n",
      "Train Epoch: 35 [0/50000 (0%)]\tLoss: 0.029808\n",
      "param norm: 1935.7696533203125\n",
      "\n",
      "Test set: Avg. loss: 0.4071, Accuracy: 9219/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 6860, 35, 35, 35, 35, 35, 6860, 35\n",
      "Lengths of input lists are 6860, 35, 35, 35, 36, 35, 6860, 36\n",
      "Total time: 948.9832241535187s\n",
      "Train Epoch: 36 [0/50000 (0%)]\tLoss: 0.011833\n",
      "param norm: 1936.7318115234375\n",
      "\n",
      "Test set: Avg. loss: 0.4149, Accuracy: 9206/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 7056, 36, 36, 36, 36, 36, 7056, 36\n",
      "Lengths of input lists are 7056, 36, 36, 36, 37, 36, 7056, 37\n",
      "Total time: 976.8420627117157s\n",
      "Train Epoch: 37 [0/50000 (0%)]\tLoss: 0.025383\n",
      "param norm: 1937.48193359375\n",
      "\n",
      "Test set: Avg. loss: 0.4242, Accuracy: 9220/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 7252, 37, 37, 37, 37, 37, 7252, 37\n",
      "Lengths of input lists are 7252, 37, 37, 37, 38, 37, 7252, 38\n",
      "Total time: 1004.0970721244812s\n",
      "Train Epoch: 38 [0/50000 (0%)]\tLoss: 0.014206\n",
      "param norm: 1938.072265625\n",
      "\n",
      "Test set: Avg. loss: 0.4271, Accuracy: 9211/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 7448, 38, 38, 38, 38, 38, 7448, 38\n",
      "Lengths of input lists are 7448, 38, 38, 38, 39, 38, 7448, 39\n",
      "Total time: 1031.6368429660797s\n",
      "Train Epoch: 39 [0/50000 (0%)]\tLoss: 0.026648\n",
      "param norm: 1938.791259765625\n",
      "\n",
      "Test set: Avg. loss: 0.4352, Accuracy: 9224/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 7644, 39, 39, 39, 39, 39, 7644, 39\n",
      "Lengths of input lists are 7644, 39, 39, 39, 40, 39, 7644, 40\n",
      "Total time: 1058.8477973937988s\n",
      "Train Epoch: 40 [0/50000 (0%)]\tLoss: 0.030663\n",
      "param norm: 1939.353759765625\n",
      "\n",
      "Test set: Avg. loss: 0.4387, Accuracy: 9224/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 7840, 40, 40, 40, 40, 40, 7840, 40\n",
      "Lengths of input lists are 7840, 40, 40, 40, 41, 40, 7840, 41\n",
      "Total time: 1086.092411994934s\n",
      "Train Epoch: 41 [0/50000 (0%)]\tLoss: 0.030857\n",
      "param norm: 1939.4302978515625\n",
      "\n",
      "Test set: Avg. loss: 0.4355, Accuracy: 9221/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 8036, 41, 41, 41, 41, 41, 8036, 41\n",
      "Lengths of input lists are 8036, 41, 41, 41, 42, 41, 8036, 42\n",
      "Total time: 1113.4044394493103s\n",
      "Train Epoch: 42 [0/50000 (0%)]\tLoss: 0.028725\n",
      "param norm: 1939.4185791015625\n",
      "\n",
      "Test set: Avg. loss: 0.4364, Accuracy: 9226/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 8232, 42, 42, 42, 42, 42, 8232, 42\n",
      "Lengths of input lists are 8232, 42, 42, 42, 43, 42, 8232, 43\n",
      "Total time: 1140.9330129623413s\n",
      "Train Epoch: 43 [0/50000 (0%)]\tLoss: 0.039358\n",
      "param norm: 1939.517578125\n",
      "\n",
      "Test set: Avg. loss: 0.4432, Accuracy: 9218/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 8428, 43, 43, 43, 43, 43, 8428, 43\n",
      "Lengths of input lists are 8428, 43, 43, 43, 44, 43, 8428, 44\n",
      "Total time: 1168.1067428588867s\n",
      "Train Epoch: 44 [0/50000 (0%)]\tLoss: 0.040147\n",
      "param norm: 1939.634765625\n",
      "\n",
      "Test set: Avg. loss: 0.4453, Accuracy: 9231/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 8624, 44, 44, 44, 44, 44, 8624, 44\n",
      "Lengths of input lists are 8624, 44, 44, 44, 45, 44, 8624, 45\n",
      "Total time: 1195.5966715812683s\n",
      "Train Epoch: 45 [0/50000 (0%)]\tLoss: 0.004172\n",
      "param norm: 1939.7967529296875\n",
      "\n",
      "Test set: Avg. loss: 0.4505, Accuracy: 9211/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 8820, 45, 45, 45, 45, 45, 8820, 45\n",
      "Lengths of input lists are 8820, 45, 45, 45, 46, 45, 8820, 46\n",
      "Total time: 1222.695826292038s\n",
      "Train Epoch: 46 [0/50000 (0%)]\tLoss: 0.003876\n",
      "param norm: 1939.884765625\n",
      "\n",
      "Test set: Avg. loss: 0.4544, Accuracy: 9222/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 9016, 46, 46, 46, 46, 46, 9016, 46\n",
      "Lengths of input lists are 9016, 46, 46, 46, 47, 46, 9016, 47\n",
      "Total time: 1250.107637643814s\n",
      "Train Epoch: 47 [0/50000 (0%)]\tLoss: 0.047279\n",
      "param norm: 1940.0126953125\n",
      "\n",
      "Test set: Avg. loss: 0.4579, Accuracy: 9227/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 9212, 47, 47, 47, 47, 47, 9212, 47\n",
      "Lengths of input lists are 9212, 47, 47, 47, 48, 47, 9212, 48\n",
      "Total time: 1277.5992209911346s\n",
      "Train Epoch: 48 [0/50000 (0%)]\tLoss: 0.019582\n",
      "param norm: 1940.1453857421875\n",
      "\n",
      "Test set: Avg. loss: 0.4550, Accuracy: 9221/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 9408, 48, 48, 48, 48, 48, 9408, 48\n",
      "Lengths of input lists are 9408, 48, 48, 48, 49, 48, 9408, 49\n",
      "Total time: 1304.6938045024872s\n",
      "Train Epoch: 49 [0/50000 (0%)]\tLoss: 0.002330\n",
      "param norm: 1940.2154541015625\n",
      "\n",
      "Test set: Avg. loss: 0.4658, Accuracy: 9224/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 9604, 49, 49, 49, 49, 49, 9604, 49\n",
      "Lengths of input lists are 9604, 49, 49, 49, 50, 49, 9604, 50\n",
      "Total time: 1332.281528711319s\n",
      "Train Epoch: 50 [0/50000 (0%)]\tLoss: 0.022492\n",
      "param norm: 1940.3978271484375\n",
      "\n",
      "Test set: Avg. loss: 0.4714, Accuracy: 9223/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 9800, 50, 50, 50, 50, 50, 9800, 50\n",
      "Lengths of input lists are 9800, 50, 50, 50, 51, 50, 9800, 51\n",
      "Total time: 1359.3936681747437s\n",
      "Doing random seed 133... \n",
      "\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "the number of parameters is 14990922\n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (32): ReLU(inplace=True)\n",
      "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (36): ReLU(inplace=True)\n",
      "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (39): ReLU(inplace=True)\n",
      "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (42): ReLU(inplace=True)\n",
      "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "=> We keep following layers in KFAC. \n",
      "(0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(7): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(8): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(9): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(11): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(13): Linear(in_features=512, out_features=512, bias=True)\n",
      "(14): Linear(in_features=512, out_features=10, bias=True)\n",
      "Started test before training\n",
      "\n",
      "Test set: Avg. loss: 2.3028, Accuracy: 1012/10000 (10%)\n",
      "\n",
      "Finished test before training in 2.0754287242889404s\n",
      "\n",
      " m_aa just after testing (should be empty if we did nto accumulate stats during traning - this si correct) {} \n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.397438\n",
      "param norm: 522.9756469726562\n",
      "\n",
      "Test set: Avg. loss: 1.7622, Accuracy: 3273/10000 (33%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 196, 1, 1, 1, 1, 1, 196, 1\n",
      "Lengths of input lists are 196, 1, 1, 1, 2, 1, 196, 2\n",
      "Total time: 26.568485975265503s\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.605203\n",
      "param norm: 561.50244140625\n",
      "\n",
      "Test set: Avg. loss: 1.9872, Accuracy: 3824/10000 (38%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 392, 2, 2, 2, 2, 2, 392, 2\n",
      "Lengths of input lists are 392, 2, 2, 2, 3, 2, 392, 3\n",
      "Total time: 53.21011805534363s\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.606139\n",
      "param norm: 562.6140747070312\n",
      "\n",
      "Test set: Avg. loss: 1.2618, Accuracy: 5832/10000 (58%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 588, 3, 3, 3, 3, 3, 588, 3\n",
      "Lengths of input lists are 588, 3, 3, 3, 4, 3, 588, 4\n",
      "Total time: 79.77221345901489s\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.306179\n",
      "param norm: 574.0369262695312\n",
      "\n",
      "Test set: Avg. loss: 1.2838, Accuracy: 6124/10000 (61%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 784, 4, 4, 4, 4, 4, 784, 4\n",
      "Lengths of input lists are 784, 4, 4, 4, 5, 4, 784, 5\n",
      "Total time: 106.44813418388367s\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.995281\n",
      "param norm: 611.4570922851562\n",
      "\n",
      "Test set: Avg. loss: 1.4236, Accuracy: 5992/10000 (60%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 980, 5, 5, 5, 5, 5, 980, 5\n",
      "Lengths of input lists are 980, 5, 5, 5, 6, 5, 980, 6\n",
      "Total time: 133.072523355484s\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 1.067109\n",
      "param norm: 676.833984375\n",
      "\n",
      "Test set: Avg. loss: 1.0449, Accuracy: 6615/10000 (66%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 1176, 6, 6, 6, 6, 6, 1176, 6\n",
      "Lengths of input lists are 1176, 6, 6, 6, 7, 6, 1176, 7\n",
      "Total time: 159.8089165687561s\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.991288\n",
      "param norm: 769.5316772460938\n",
      "\n",
      "Test set: Avg. loss: 0.7542, Accuracy: 7671/10000 (77%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 1372, 7, 7, 7, 7, 7, 1372, 7\n",
      "Lengths of input lists are 1372, 7, 7, 7, 8, 7, 1372, 8\n",
      "Total time: 186.66349625587463s\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.619591\n",
      "param norm: 893.4158935546875\n",
      "\n",
      "Test set: Avg. loss: 0.8852, Accuracy: 7247/10000 (72%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 1568, 8, 8, 8, 8, 8, 1568, 8\n",
      "Lengths of input lists are 1568, 8, 8, 8, 9, 8, 1568, 9\n",
      "Total time: 213.26360058784485s\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.704941\n",
      "param norm: 1040.4794921875\n",
      "\n",
      "Test set: Avg. loss: 0.5967, Accuracy: 8121/10000 (81%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 1764, 9, 9, 9, 9, 9, 1764, 9\n",
      "Lengths of input lists are 1764, 9, 9, 9, 10, 9, 1764, 10\n",
      "Total time: 239.96759963035583s\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.359623\n",
      "param norm: 1206.1236572265625\n",
      "\n",
      "Test set: Avg. loss: 0.6271, Accuracy: 8077/10000 (81%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 1960, 10, 10, 10, 10, 10, 1960, 10\n",
      "Lengths of input lists are 1960, 10, 10, 10, 11, 10, 1960, 11\n",
      "Total time: 266.64488339424133s\n",
      "Train Epoch: 11 [0/50000 (0%)]\tLoss: 0.535722\n",
      "param norm: 1397.2109375\n",
      "\n",
      "Test set: Avg. loss: 0.4720, Accuracy: 8520/10000 (85%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 2156, 11, 11, 11, 11, 11, 2156, 11\n",
      "Lengths of input lists are 2156, 11, 11, 11, 12, 11, 2156, 12\n",
      "Total time: 293.27209973335266s\n",
      "Train Epoch: 12 [0/50000 (0%)]\tLoss: 0.434016\n",
      "param norm: 1590.154541015625\n",
      "\n",
      "Test set: Avg. loss: 0.5663, Accuracy: 8229/10000 (82%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 2352, 12, 12, 12, 12, 12, 2352, 12\n",
      "Lengths of input lists are 2352, 12, 12, 12, 13, 12, 2352, 13\n",
      "Total time: 319.94015550613403s\n",
      "Train Epoch: 13 [0/50000 (0%)]\tLoss: 0.443700\n",
      "param norm: 1778.348388671875\n",
      "\n",
      "Test set: Avg. loss: 0.3186, Accuracy: 8985/10000 (90%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 2548, 13, 13, 13, 13, 13, 2548, 13\n",
      "Lengths of input lists are 2548, 13, 13, 13, 14, 13, 2548, 14\n",
      "Total time: 346.2505054473877s\n",
      "Train Epoch: 14 [0/50000 (0%)]\tLoss: 0.183636\n",
      "param norm: 1793.619140625\n",
      "\n",
      "Test set: Avg. loss: 0.3018, Accuracy: 9043/10000 (90%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 2744, 14, 14, 14, 14, 14, 2744, 14\n",
      "Lengths of input lists are 2744, 14, 14, 14, 15, 14, 2744, 15\n",
      "Total time: 372.89184832572937s\n",
      "Train Epoch: 15 [0/50000 (0%)]\tLoss: 0.147549\n",
      "param norm: 1812.2974853515625\n",
      "\n",
      "Test set: Avg. loss: 0.3080, Accuracy: 9045/10000 (90%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 2940, 15, 15, 15, 15, 15, 2940, 15\n",
      "Lengths of input lists are 2940, 15, 15, 15, 16, 15, 2940, 16\n",
      "Total time: 399.4578547477722s\n",
      "Train Epoch: 16 [0/50000 (0%)]\tLoss: 0.152021\n",
      "param norm: 1831.797119140625\n",
      "\n",
      "Test set: Avg. loss: 0.3161, Accuracy: 9064/10000 (91%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 3136, 16, 16, 16, 16, 16, 3136, 16\n",
      "Lengths of input lists are 3136, 16, 16, 16, 17, 16, 3136, 17\n",
      "Total time: 426.108051776886s\n",
      "Train Epoch: 17 [0/50000 (0%)]\tLoss: 0.107199\n",
      "param norm: 1851.86083984375\n",
      "\n",
      "Test set: Avg. loss: 0.3134, Accuracy: 9091/10000 (91%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 3332, 17, 17, 17, 17, 17, 3332, 17\n",
      "Lengths of input lists are 3332, 17, 17, 17, 18, 17, 3332, 18\n",
      "Total time: 452.8669400215149s\n",
      "Train Epoch: 18 [0/50000 (0%)]\tLoss: 0.128648\n",
      "param norm: 1870.87451171875\n",
      "\n",
      "Test set: Avg. loss: 0.2933, Accuracy: 9177/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 3528, 18, 18, 18, 18, 18, 3528, 18\n",
      "Lengths of input lists are 3528, 18, 18, 18, 19, 18, 3528, 19\n",
      "Total time: 479.459011554718s\n",
      "Train Epoch: 19 [0/50000 (0%)]\tLoss: 0.079783\n",
      "param norm: 1872.909912109375\n",
      "\n",
      "Test set: Avg. loss: 0.3015, Accuracy: 9173/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 3724, 19, 19, 19, 19, 19, 3724, 19\n",
      "Lengths of input lists are 3724, 19, 19, 19, 20, 19, 3724, 20\n",
      "Total time: 506.26569509506226s\n",
      "Train Epoch: 20 [0/50000 (0%)]\tLoss: 0.093083\n",
      "param norm: 1874.568359375\n",
      "\n",
      "Test set: Avg. loss: 0.3025, Accuracy: 9205/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 3920, 20, 20, 20, 20, 20, 3920, 20\n",
      "Lengths of input lists are 3920, 20, 20, 20, 21, 20, 3920, 21\n",
      "Total time: 534.2072665691376s\n",
      "Train Epoch: 21 [0/50000 (0%)]\tLoss: 0.087422\n",
      "param norm: 1876.738525390625\n",
      "\n",
      "Test set: Avg. loss: 0.3139, Accuracy: 9186/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 4116, 21, 21, 21, 21, 21, 4116, 21\n",
      "Lengths of input lists are 4116, 21, 21, 21, 22, 21, 4116, 22\n",
      "Total time: 563.0380675792694s\n",
      "Train Epoch: 22 [0/50000 (0%)]\tLoss: 0.102678\n",
      "param norm: 1878.701904296875\n",
      "\n",
      "Test set: Avg. loss: 0.3238, Accuracy: 9157/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 4312, 22, 22, 22, 22, 22, 4312, 22\n",
      "Lengths of input lists are 4312, 22, 22, 22, 23, 22, 4312, 23\n",
      "Total time: 590.4791836738586s\n",
      "Train Epoch: 23 [0/50000 (0%)]\tLoss: 0.036518\n",
      "param norm: 1880.325439453125\n",
      "\n",
      "Test set: Avg. loss: 0.3225, Accuracy: 9193/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 4508, 23, 23, 23, 23, 23, 4508, 23\n",
      "Lengths of input lists are 4508, 23, 23, 23, 24, 23, 4508, 24\n",
      "Total time: 618.0411930084229s\n",
      "Train Epoch: 24 [0/50000 (0%)]\tLoss: 0.067689\n",
      "param norm: 1882.208984375\n",
      "\n",
      "Test set: Avg. loss: 0.3313, Accuracy: 9178/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 4704, 24, 24, 24, 24, 24, 4704, 24\n",
      "Lengths of input lists are 4704, 24, 24, 24, 25, 24, 4704, 25\n",
      "Total time: 645.5515546798706s\n",
      "Train Epoch: 25 [0/50000 (0%)]\tLoss: 0.048662\n",
      "param norm: 1884.08154296875\n",
      "\n",
      "Test set: Avg. loss: 0.3319, Accuracy: 9221/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 4900, 25, 25, 25, 25, 25, 4900, 25\n",
      "Lengths of input lists are 4900, 25, 25, 25, 26, 25, 4900, 26\n",
      "Total time: 672.9473831653595s\n",
      "Train Epoch: 26 [0/50000 (0%)]\tLoss: 0.044519\n",
      "param norm: 1887.6658935546875\n",
      "\n",
      "Test set: Avg. loss: 0.3505, Accuracy: 9200/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 5096, 26, 26, 26, 26, 26, 5096, 26\n",
      "Lengths of input lists are 5096, 26, 26, 26, 27, 26, 5096, 27\n",
      "Total time: 700.1205413341522s\n",
      "Train Epoch: 27 [0/50000 (0%)]\tLoss: 0.048452\n",
      "param norm: 1891.39404296875\n",
      "\n",
      "Test set: Avg. loss: 0.3453, Accuracy: 9219/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 5292, 27, 27, 27, 27, 27, 5292, 27\n",
      "Lengths of input lists are 5292, 27, 27, 27, 28, 27, 5292, 28\n",
      "Total time: 727.5380809307098s\n",
      "Train Epoch: 28 [0/50000 (0%)]\tLoss: 0.031162\n",
      "param norm: 1891.6302490234375\n",
      "\n",
      "Test set: Avg. loss: 0.3536, Accuracy: 9215/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 5488, 28, 28, 28, 28, 28, 5488, 28\n",
      "Lengths of input lists are 5488, 28, 28, 28, 29, 28, 5488, 29\n",
      "Total time: 754.7155256271362s\n",
      "Train Epoch: 29 [0/50000 (0%)]\tLoss: 0.063063\n",
      "param norm: 1892.16552734375\n",
      "\n",
      "Test set: Avg. loss: 0.3559, Accuracy: 9224/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 5684, 29, 29, 29, 29, 29, 5684, 29\n",
      "Lengths of input lists are 5684, 29, 29, 29, 30, 29, 5684, 30\n",
      "Total time: 782.2822594642639s\n",
      "Train Epoch: 30 [0/50000 (0%)]\tLoss: 0.013619\n",
      "param norm: 1892.4786376953125\n",
      "\n",
      "Test set: Avg. loss: 0.3607, Accuracy: 9210/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 5880, 30, 30, 30, 30, 30, 5880, 30\n",
      "Lengths of input lists are 5880, 30, 30, 30, 31, 30, 5880, 31\n",
      "Total time: 809.5388696193695s\n",
      "Train Epoch: 31 [0/50000 (0%)]\tLoss: 0.015764\n",
      "param norm: 1892.817626953125\n",
      "\n",
      "Test set: Avg. loss: 0.3742, Accuracy: 9218/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 6076, 31, 31, 31, 31, 31, 6076, 31\n",
      "Lengths of input lists are 6076, 31, 31, 31, 32, 31, 6076, 32\n",
      "Total time: 837.1429364681244s\n",
      "Train Epoch: 32 [0/50000 (0%)]\tLoss: 0.014541\n",
      "param norm: 1893.2684326171875\n",
      "\n",
      "Test set: Avg. loss: 0.3722, Accuracy: 9217/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 6272, 32, 32, 32, 32, 32, 6272, 32\n",
      "Lengths of input lists are 6272, 32, 32, 32, 33, 32, 6272, 33\n",
      "Total time: 864.7336747646332s\n",
      "Train Epoch: 33 [0/50000 (0%)]\tLoss: 0.016153\n",
      "param norm: 1893.6898193359375\n",
      "\n",
      "Test set: Avg. loss: 0.3846, Accuracy: 9218/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 6468, 33, 33, 33, 33, 33, 6468, 33\n",
      "Lengths of input lists are 6468, 33, 33, 33, 34, 33, 6468, 34\n",
      "Total time: 891.9731373786926s\n",
      "Train Epoch: 34 [0/50000 (0%)]\tLoss: 0.017442\n",
      "param norm: 1894.0517578125\n",
      "\n",
      "Test set: Avg. loss: 0.3867, Accuracy: 9222/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 6664, 34, 34, 34, 34, 34, 6664, 34\n",
      "Lengths of input lists are 6664, 34, 34, 34, 35, 34, 6664, 35\n",
      "Total time: 919.4420673847198s\n",
      "Train Epoch: 35 [0/50000 (0%)]\tLoss: 0.011844\n",
      "param norm: 1894.3681640625\n",
      "\n",
      "Test set: Avg. loss: 0.4033, Accuracy: 9233/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 6860, 35, 35, 35, 35, 35, 6860, 35\n",
      "Lengths of input lists are 6860, 35, 35, 35, 36, 35, 6860, 36\n",
      "Total time: 946.71053814888s\n",
      "Train Epoch: 36 [0/50000 (0%)]\tLoss: 0.002860\n",
      "param norm: 1895.3033447265625\n",
      "\n",
      "Test set: Avg. loss: 0.4152, Accuracy: 9208/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 7056, 36, 36, 36, 36, 36, 7056, 36\n",
      "Lengths of input lists are 7056, 36, 36, 36, 37, 36, 7056, 37\n",
      "Total time: 974.3290169239044s\n",
      "Train Epoch: 37 [0/50000 (0%)]\tLoss: 0.008732\n",
      "param norm: 1895.93798828125\n",
      "\n",
      "Test set: Avg. loss: 0.4305, Accuracy: 9222/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 7252, 37, 37, 37, 37, 37, 7252, 37\n",
      "Lengths of input lists are 7252, 37, 37, 37, 38, 37, 7252, 38\n",
      "Total time: 1001.7004497051239s\n",
      "Train Epoch: 38 [0/50000 (0%)]\tLoss: 0.006630\n",
      "param norm: 1896.8040771484375\n",
      "\n",
      "Test set: Avg. loss: 0.4370, Accuracy: 9233/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 7448, 38, 38, 38, 38, 38, 7448, 38\n",
      "Lengths of input lists are 7448, 38, 38, 38, 39, 38, 7448, 39\n",
      "Total time: 1029.201355934143s\n",
      "Train Epoch: 39 [0/50000 (0%)]\tLoss: 0.007746\n",
      "param norm: 1897.3616943359375\n",
      "\n",
      "Test set: Avg. loss: 0.4384, Accuracy: 9226/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 7644, 39, 39, 39, 39, 39, 7644, 39\n",
      "Lengths of input lists are 7644, 39, 39, 39, 40, 39, 7644, 40\n",
      "Total time: 1056.5880591869354s\n",
      "Train Epoch: 40 [0/50000 (0%)]\tLoss: 0.015578\n",
      "param norm: 1898.015380859375\n",
      "\n",
      "Test set: Avg. loss: 0.4402, Accuracy: 9231/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 7840, 40, 40, 40, 40, 40, 7840, 40\n",
      "Lengths of input lists are 7840, 40, 40, 40, 41, 40, 7840, 41\n",
      "Total time: 1084.1956293582916s\n",
      "Train Epoch: 41 [0/50000 (0%)]\tLoss: 0.054563\n",
      "param norm: 1898.1337890625\n",
      "\n",
      "Test set: Avg. loss: 0.4416, Accuracy: 9241/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 8036, 41, 41, 41, 41, 41, 8036, 41\n",
      "Lengths of input lists are 8036, 41, 41, 41, 42, 41, 8036, 42\n",
      "Total time: 1111.4072391986847s\n",
      "Train Epoch: 42 [0/50000 (0%)]\tLoss: 0.039383\n",
      "param norm: 1898.234375\n",
      "\n",
      "Test set: Avg. loss: 0.4444, Accuracy: 9237/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 8232, 42, 42, 42, 42, 42, 8232, 42\n",
      "Lengths of input lists are 8232, 42, 42, 42, 43, 42, 8232, 43\n",
      "Total time: 1138.9896330833435s\n",
      "Train Epoch: 43 [0/50000 (0%)]\tLoss: 0.011085\n",
      "param norm: 1898.330810546875\n",
      "\n",
      "Test set: Avg. loss: 0.4518, Accuracy: 9235/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 8428, 43, 43, 43, 43, 43, 8428, 43\n",
      "Lengths of input lists are 8428, 43, 43, 43, 44, 43, 8428, 44\n",
      "Total time: 1166.2763938903809s\n",
      "Train Epoch: 44 [0/50000 (0%)]\tLoss: 0.034311\n",
      "param norm: 1898.5205078125\n",
      "\n",
      "Test set: Avg. loss: 0.4489, Accuracy: 9242/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 8624, 44, 44, 44, 44, 44, 8624, 44\n",
      "Lengths of input lists are 8624, 44, 44, 44, 45, 44, 8624, 45\n",
      "Total time: 1193.9339175224304s\n",
      "Train Epoch: 45 [0/50000 (0%)]\tLoss: 0.022267\n",
      "param norm: 1898.6798095703125\n",
      "\n",
      "Test set: Avg. loss: 0.4590, Accuracy: 9234/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 8820, 45, 45, 45, 45, 45, 8820, 45\n",
      "Lengths of input lists are 8820, 45, 45, 45, 46, 45, 8820, 46\n",
      "Total time: 1221.1246523857117s\n",
      "Train Epoch: 46 [0/50000 (0%)]\tLoss: 0.007979\n",
      "param norm: 1898.8892822265625\n",
      "\n",
      "Test set: Avg. loss: 0.4680, Accuracy: 9237/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 9016, 46, 46, 46, 46, 46, 9016, 46\n",
      "Lengths of input lists are 9016, 46, 46, 46, 47, 46, 9016, 47\n",
      "Total time: 1248.6446554660797s\n",
      "Train Epoch: 47 [0/50000 (0%)]\tLoss: 0.023570\n",
      "param norm: 1899.061279296875\n",
      "\n",
      "Test set: Avg. loss: 0.4658, Accuracy: 9232/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 9212, 47, 47, 47, 47, 47, 9212, 47\n",
      "Lengths of input lists are 9212, 47, 47, 47, 48, 47, 9212, 48\n",
      "Total time: 1276.1946940422058s\n",
      "Train Epoch: 48 [0/50000 (0%)]\tLoss: 0.002166\n",
      "param norm: 1899.1839599609375\n",
      "\n",
      "Test set: Avg. loss: 0.4723, Accuracy: 9237/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 9408, 48, 48, 48, 48, 48, 9408, 48\n",
      "Lengths of input lists are 9408, 48, 48, 48, 49, 48, 9408, 49\n",
      "Total time: 1303.4393472671509s\n",
      "Train Epoch: 49 [0/50000 (0%)]\tLoss: 0.008799\n",
      "param norm: 1899.38134765625\n",
      "\n",
      "Test set: Avg. loss: 0.4716, Accuracy: 9235/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 9604, 49, 49, 49, 49, 49, 9604, 49\n",
      "Lengths of input lists are 9604, 49, 49, 49, 50, 49, 9604, 50\n",
      "Total time: 1330.9798481464386s\n",
      "Train Epoch: 50 [0/50000 (0%)]\tLoss: 0.047820\n",
      "param norm: 1899.4912109375\n",
      "\n",
      "Test set: Avg. loss: 0.4704, Accuracy: 9235/10000 (92%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 9800, 50, 50, 50, 50, 50, 9800, 50\n",
      "Lengths of input lists are 9800, 50, 50, 50, 51, 50, 9800, 51\n",
      "Total time: 1358.3409266471863s\n",
      "Doing random seed 134... \n",
      "\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "the number of parameters is 14990922\n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (32): ReLU(inplace=True)\n",
      "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (36): ReLU(inplace=True)\n",
      "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (39): ReLU(inplace=True)\n",
      "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (42): ReLU(inplace=True)\n",
      "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "=> We keep following layers in KFAC. \n",
      "(0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(7): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(8): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(9): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(11): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(13): Linear(in_features=512, out_features=512, bias=True)\n",
      "(14): Linear(in_features=512, out_features=10, bias=True)\n",
      "Started test before training\n",
      "\n",
      "Test set: Avg. loss: 2.3028, Accuracy: 1133/10000 (11%)\n",
      "\n",
      "Finished test before training in 2.0875320434570312s\n",
      "\n",
      " m_aa just after testing (should be empty if we did nto accumulate stats during traning - this si correct) {} \n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.362995\n",
      "param norm: 522.9937744140625\n",
      "\n",
      "Test set: Avg. loss: 1.8954, Accuracy: 3081/10000 (31%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 196, 1, 1, 1, 1, 1, 196, 1\n",
      "Lengths of input lists are 196, 1, 1, 1, 2, 1, 196, 2\n",
      "Total time: 26.75047516822815s\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.736707\n",
      "param norm: 560.0584106445312\n",
      "\n",
      "Test set: Avg. loss: 2.3525, Accuracy: 3293/10000 (33%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 392, 2, 2, 2, 2, 2, 392, 2\n",
      "Lengths of input lists are 392, 2, 2, 2, 3, 2, 392, 3\n",
      "Total time: 53.559195041656494s\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.477948\n",
      "param norm: 560.0563354492188\n",
      "\n",
      "Test set: Avg. loss: 1.5897, Accuracy: 4949/10000 (49%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 588, 3, 3, 3, 3, 3, 588, 3\n",
      "Lengths of input lists are 588, 3, 3, 3, 4, 3, 588, 4\n",
      "Total time: 80.36985039710999s\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.191914\n",
      "param norm: 572.5137329101562\n",
      "\n",
      "Test set: Avg. loss: 1.1074, Accuracy: 6574/10000 (66%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 784, 4, 4, 4, 4, 4, 784, 4\n",
      "Lengths of input lists are 784, 4, 4, 4, 5, 4, 784, 5\n",
      "Total time: 107.23120474815369s\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.918502\n",
      "param norm: 609.6275634765625\n",
      "\n",
      "Test set: Avg. loss: 1.0760, Accuracy: 6825/10000 (68%)\n",
      "\n",
      "Saving data...\n",
      " Lengths of SAVED lists are 980, 5, 5, 5, 5, 5, 980, 5\n",
      "Lengths of input lists are 980, 5, 5, 5, 6, 5, 980, 6\n",
      "Total time: 133.99300384521484s\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 1.065191\n",
      "param norm: 674.9967651367188\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "torch.multiprocessing.set_start_method('spawn')\n",
    "\n",
    "torch.backends.cudnn.enabled = True # False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# ------------------------------\n",
    "# ---- Training parameters -----\n",
    "n_epochs = 50\n",
    "opti_type = 'SRE_KFAC'\n",
    "# l_rate = 0.01;\n",
    "def l_rate_function(epoch_n, iter_n):\n",
    "    if epoch_n == 1:\n",
    "        if iter_n < 3:\n",
    "            return 0.3\n",
    "        else:\n",
    "            return 0.3\n",
    "    elif epoch_n == 2:\n",
    "        return 0.2\n",
    "    elif epoch_n >= 3 and epoch_n < 7:\n",
    "        return 0.1\n",
    "    elif epoch_n >= 7 and epoch_n < 13:\n",
    "        return 0.1\n",
    "    elif epoch_n >= 13 and epoch_n < 18:\n",
    "        return 0.03\n",
    "    elif epoch_n >= 18 and epoch_n < 27:\n",
    "        return 0.01\n",
    "    elif epoch_n >= 27 and epoch_n < 40:\n",
    "        return 0.003\n",
    "    elif epoch_n >= 40:\n",
    "        return 0.001\n",
    "\n",
    "kfac_clip = 7e-2; #KFAC_damping = 1e-01 #3e-02; \n",
    "stat_decay = 0.95 #0.95\n",
    "\n",
    "momentum = 0.0\n",
    "WD = 7e-4 \n",
    "lambdaa = 0.0 #1#1#7 #0.007\n",
    "batch_size_train = batch_size_test = 256\n",
    "# ====================================================\n",
    "\n",
    "###################################### SCHEDULES ###############################\n",
    "# dict to have schedule! eys are epochs: key map to frequency, stuff only changes at keys and then stays constant.\n",
    "KFAC_matrix_update_frequency_dict = {0: 10, 5: 10, 10: 10, 20: 10, 22: 10, 50: 10}\n",
    "KFAC_matrix_invert_frequency_dict = {0: 50, 5: 50, 10: 50, 20: 30, 22: 30, 50: 30}\n",
    "\n",
    "KFAC_damping_dict = {0: 1e-01, 7: 1e-01, 25: 5e-02, 35: 1e-02}\n",
    "\n",
    "rsvd_rank_dict = {0: 220, 7:220, 15:230, 20: 230, 22: 230, 30:230, 40:230}\n",
    "oversampling_parameter_dict = {0: 10, 7:10, 10:10, 20: 10, 22: 11, 30:12, 40:12}\n",
    "rsvd_niter_dict = {0: 4, 7:4, 10:4} #, 20: 10, 25: 18, 30:10, 40:10}\n",
    "\n",
    "################################ END SCHEDULES #################################\n",
    "\n",
    "KFAC_matrix_update_frequency = KFAC_matrix_update_frequency_dict[0]\n",
    "KFAC_matrix_invert_frequency = KFAC_matrix_invert_frequency_dict[0]\n",
    "KFAC_damping = KFAC_damping_dict[0]\n",
    "rsvd_rank = rsvd_rank_dict[0]\n",
    "oversampling_parameter = oversampling_parameter_dict[0]\n",
    "rsvd_niter = rsvd_niter_dict[0]\n",
    "\n",
    "# ONLY FOR SAVED FILE NAME: beta1 and beta2 are just 2 channels for filename ==\n",
    "beta1 = WD\n",
    "beta2 = KFAC_damping\n",
    "\n",
    "log_interval = 500000000 #120 #int(200 *batch_size_train/8192)\n",
    "basic_path = '/content/gdrive/My Drive/FC_NET_project/results_{}'.format(opti_type)\n",
    "error_write_path = '/content/gdrive/My Drive/P_data/Errors/err_{}'.format(opti_type)\n",
    "\n",
    "#rsvd paramaters\n",
    "for random_seed in [131, 132, 133, 134, 135, 136, 137, 138, 139,140]: #[31,32,33,34,35,36,37,38,39,40]:\n",
    "  print('Doing random seed {}... \\n'.format(random_seed))\n",
    "  torch.manual_seed(random_seed)\n",
    "\n",
    "  #------------------------------------------------------------------------------\n",
    "  #--------------------------- DATA LOADERS -------------------------------------\n",
    "  def collation_fct(x):\n",
    "    return  tuple(x_.to(cuda0) for x_ in default_collate(x))\n",
    "  #------------------------------------------------------------------------------\n",
    "\n",
    " \n",
    "  train_loader, test_loader = get_dataloader(dataset = 'cifar10', train_batch_size = batch_size_train,\n",
    "                                          test_batch_size = batch_size_test, num_workers = 0, \n",
    "                                          collation_fct = collation_fct, root='./data_CIFAR10')\n",
    "  # INCLUDES DATA AUGMENTATION!\n",
    "\n",
    "  classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "  #------------------------------------------------------------------------------\n",
    "  #-------------------------------- building the NET ----------------------------\n",
    "\n",
    "  ###### VGG16 using the construction on alecwangcq github in KFAC project ###############\n",
    "  network = get_network('vgg16_bn',\n",
    "                    #depth = 19,\n",
    "                    num_classes = 10,\n",
    "                    #growthRate = 12,\n",
    "                    #compressionRate = 2,\n",
    "                    widen_factor = 1)#,\n",
    "                    #dropRate = 0.05)\n",
    "\n",
    "  ############## change the classifier to be 2 linear layers ################\n",
    "  ######################### s.t. we can add dropout #########################\n",
    "  network.classifier = nn.Sequential(\n",
    "      nn.Linear(512, 512),\n",
    "      nn.ReLU(True),\n",
    "      nn.Dropout(), # p =0.5  by default\n",
    "      nn.Linear(512, 10),\n",
    "  )\n",
    "\n",
    "  network.to(cuda0)\n",
    "  softmax_crossentropy = nn.CrossEntropyLoss() # since the net returns non-softmaxed output we need to use this loss which is softmax crossentropy\n",
    "  softmax_crossentropy_test = nn.CrossEntropyLoss(size_average = False)\n",
    "  #network.load_state_dict(torch.load('./model_0.pth'))\n",
    "  print('the number of parameters is {}'.format(sum([p.numel() for p in network.parameters()])))\n",
    "  # -------------------------------------------\n",
    "\n",
    "  # --------------------------------- DEFINE THE OPTIMISER ----------------------\n",
    "  # -----------------------------------------------------------------------------\n",
    "  def regularized_loss_fct(output,target,network, lambdaa):\n",
    "      KFAC_matrix_loss = softmax_crossentropy(output,target)\n",
    "      l2_reg = torch.tensor(0., device = cuda0)\n",
    "      for param in network.parameters():\n",
    "          l2_reg += torch.norm(param)\n",
    "      KFAC_matrix_loss += lambdaa * l2_reg\n",
    "      \n",
    "      loss_for_gradient = softmax_crossentropy(output,target)\n",
    "      l2_reg = torch.tensor(0., device = cuda0)\n",
    "      for param in network.parameters():\n",
    "          l2_reg += torch.norm(param)\n",
    "      loss_for_gradient += lambdaa * l2_reg\n",
    "      \n",
    "      #print('the parameter vector norm is {}'.format(l2_reg))\n",
    "      return KFAC_matrix_loss, loss_for_gradient, l2_reg\n",
    "\n",
    "\n",
    "  optimizer = SRE_KFACOptimizer(network, lr_function = l_rate_function, momentum = momentum, stat_decay = stat_decay, \n",
    "                            kl_clip = kfac_clip, damping = KFAC_damping, weight_decay = WD, TCov = KFAC_matrix_update_frequency,\n",
    "                            TInv = KFAC_matrix_invert_frequency, \n",
    "                            rsvd_rank = rsvd_rank, oversampling_parameter = oversampling_parameter, rsvd_niter = rsvd_niter)\n",
    "\n",
    "  scheduler = None\n",
    "\n",
    "  # ------------------------------------------------------------------------------\n",
    "\n",
    "  # track performance of parameters and progress\n",
    "  train_losses = []; train_losses_per_epoch = []; train_accuracy_per_epoch = []\n",
    "\n",
    "  train_accuracy = []; test_accuracy = []; time_per_epoch_ = []; time_per_iter = []\n",
    "  train_counter = []; test_losses = []\n",
    "  test_counter = [i * len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
    "\n",
    "  ##initial network\n",
    "\n",
    "  # torch.save(network.state_dict(), './resultsSGD/model.pth')\n",
    "  def save_data_():\n",
    "        # basic_path = os.path.join(basic_path, '/')\n",
    "        train_losses_cpu = []; train_losses_per_epoch_cpu = []; train_accuracy_per_epoch_cpu = []\n",
    "        train_accuracy_cpu = []; test_accuracy_cpu = []; time_per_epoch__cpu = []; time_per_iter_cpu = []\n",
    "        test_losses_cpu = []\n",
    "\n",
    "        '''PER-EPOCH GUYS CONVERSION TO CPU'''\n",
    "        for trlpe,traccpe,timpe,teacc,tel in zip(train_losses_per_epoch, train_accuracy_per_epoch, time_per_epoch_, \n",
    "                                                              test_accuracy, test_losses):\n",
    "          train_losses_per_epoch_cpu.append(trlpe.cpu()); train_accuracy_per_epoch_cpu.append(traccpe.cpu()); \n",
    "          time_per_epoch__cpu.append(timpe)\n",
    "          test_accuracy_cpu.append(teacc.cpu()); test_losses_cpu.append(tel.cpu()); # these are per epoch by definition\n",
    "        \n",
    "        '''PER-ITERATION GUYS CONVERSION TO CPU'''\n",
    "        for trl,tim in zip(train_losses, time_per_iter):\n",
    "          train_losses_cpu.append(trl.cpu()); time_per_iter_cpu.append(tim);\n",
    "        for tracc in train_accuracy: # apperently train accuracy per iter is wrongly saved only per epoch.... because it's saved only when logged\n",
    "          train_accuracy_cpu.append(tracc.cpu());\n",
    "        \n",
    "        print('Saving data...\\n Lengths of SAVED lists are {}, {}, {}, {}, {}, {}, {}, {}'.format(len(train_losses_cpu), len(train_losses_per_epoch_cpu), len(train_accuracy_per_epoch_cpu),\n",
    "                                                                          len(train_accuracy_cpu), len(test_accuracy_cpu), len(time_per_epoch__cpu),\n",
    "                                                                          len(time_per_iter_cpu), len(test_losses_cpu) ))\n",
    "        print('Lengths of input lists are {}, {}, {}, {}, {}, {}, {}, {}'.format(len(train_losses), len(train_losses_per_epoch), len(train_accuracy_per_epoch),\n",
    "                                                                          len(train_accuracy), len(test_accuracy), len(time_per_epoch_),\n",
    "                                                                          len(time_per_iter), len(test_losses) ))\n",
    "        np.save(os.path.join(basic_path,\n",
    "                            '{}_{}_{}_{}_{}_{}_run{}_{}'.format(opti_type, batch_size_train,\n",
    "                                                        beta1,\n",
    "                                                        beta2, l_rate_function(40,40), stat_decay, \n",
    "                                                        random_seed, 'train_losses')),\n",
    "                train_losses_cpu)\n",
    "        np.save(os.path.join(basic_path,\n",
    "                            '{}_{}_{}_{}_{}_{}_run{}_{}'.format(opti_type, batch_size_train,\n",
    "                                                        beta1,\n",
    "                                                        beta2, l_rate_function(40,40), stat_decay,\n",
    "                                                        random_seed, 'time_per_iter')),\n",
    "                time_per_iter_cpu)\n",
    "\n",
    "        np.save(os.path.join(basic_path,\n",
    "                            '{}_{}_{}_{}_{}_{}_run{}_{}'.format(opti_type, batch_size_train,\n",
    "                                                        beta1,\n",
    "                                                        beta2, l_rate_function(40,40), stat_decay ,\n",
    "                                                        random_seed, 'train_losses_per_epoch')),\n",
    "                train_losses_per_epoch_cpu)\n",
    "        np.save(os.path.join(basic_path,\n",
    "                            '{}_{}_{}_{}_{}_{}_run{}_{}'.format(opti_type, batch_size_train,\n",
    "                                                        beta1,\n",
    "                                                        beta2, l_rate_function(40,40), stat_decay, \n",
    "                                                        random_seed, 'train_accuracy')),\n",
    "                train_accuracy_cpu)\n",
    "        np.save(os.path.join(basic_path,\n",
    "                            '{}_{}_{}_{}_{}_{}_run{}_{}'.format(opti_type, batch_size_train,\n",
    "                                                        beta1,\n",
    "                                                        beta2, l_rate_function(40,40), stat_decay,\n",
    "                                                        random_seed, 'train_accuracy_per_epoch')),\n",
    "                train_accuracy_per_epoch_cpu)\n",
    "        np.save(os.path.join(basic_path,\n",
    "                            '{}_{}_{}_{}_{}_{}_run{}_{}'.format(opti_type, batch_size_train,\n",
    "                                                        beta1,\n",
    "                                                        beta2, l_rate_function(40,40), stat_decay, \n",
    "                                                        random_seed, 'time_per_epoch_')),\n",
    "                time_per_epoch__cpu)\n",
    "        np.save(os.path.join(basic_path,\n",
    "                            '{}_{}_{}_{}_{}_{}_run{}_{}'.format(opti_type, batch_size_train,\n",
    "                                                        beta1,\n",
    "                                                        beta2, l_rate_function(40,40), stat_decay,\n",
    "                                                        random_seed, 'test_accuracy')),\n",
    "                test_accuracy_cpu)\n",
    "        np.save(os.path.join(basic_path,\n",
    "                            '{}_{}_{}_{}_{}_{}_run{}_{}'.format(opti_type, batch_size_train,\n",
    "                                                        beta1,\n",
    "                                                        beta2, l_rate_function(40,40), stat_decay, \n",
    "                                                        random_seed, 'test_losses')),\n",
    "                test_losses_cpu)\n",
    "          \n",
    "  def train(epoch, step_counter, log_interval = log_interval):\n",
    "      network.train()\n",
    "      correct = 0\n",
    "      time_epoch = 0\n",
    "      optimizer.epoch_number = epoch\n",
    "      # previous_step = np.array([0])\n",
    "      for batch_idx, (data, target) in enumerate(train_loader):\n",
    "          step_counter = step_counter + 1\n",
    "          start = time.time()\n",
    "          optimizer.zero_grad()\n",
    "          #data = data.double()\n",
    "          output = network(data)\n",
    "          pred = output.data.max(1, keepdim=True)[1]\n",
    "          correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "\n",
    "          KFAC_matrix_loss, loss_for_gradient, l2_reg = regularized_loss_fct(output, target, network, lambdaa)\n",
    "          #t1 = time.time()\n",
    "          \n",
    "          # update network weights\n",
    "          optimizer.zero_grad()\n",
    "          \n",
    "          ''' assemble <KFAC matrix> loss to compute KFAC matrix'''\n",
    "          if optimizer.steps % KFAC_matrix_update_frequency == 0:\n",
    "              optimizer.acc_stats = True\n",
    "              KFAC_matrix_loss.backward(retain_graph=True)\n",
    "          optimizer.acc_stats = False\n",
    "      \n",
    "          ''' compute gradient of <Policy loss> (precond by KFAC^{-1}) and then take step'''\n",
    "          ''' also need to compute and return the gradient for TRUEish F^{-1}g computation'''\n",
    "          optimizer.zero_grad()\n",
    "          loss_for_gradient.backward()\n",
    "          KFAC_direction = optimizer.step(epoch, error_write_path)\n",
    "          loss_value = loss_for_gradient.detach()\n",
    "          end = time.time()\n",
    "\n",
    "          time_per_iter.append(start - end)\n",
    "          time_epoch = time_epoch + (end - start)\n",
    "          train_losses.append(loss_value)\n",
    "          if scheduler == None:\n",
    "              pass\n",
    "          else:\n",
    "              scheduler.step()\n",
    "\n",
    "          if batch_idx % log_interval == 0:\n",
    "              train_counter.append(\n",
    "                  (batch_idx * 64) + ((epoch - 1) * len(train_loader.dataset)))\n",
    "              # change the saving path\n",
    "              print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                  epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                        100. * batch_idx / len(train_loader), loss_value))\n",
    "              \n",
    "              print('param norm: {}'.format(l2_reg))\n",
    "\n",
    "      accc = 100. * correct / len(train_loader.dataset)\n",
    "      train_accuracy.append(accc)\n",
    "      time_per_epoch_.append(time_epoch)\n",
    "      train_losses_per_epoch.append(loss_value)\n",
    "      train_accuracy_per_epoch.append(accc)\n",
    "      return step_counter\n",
    "\n",
    "  def test():\n",
    "      network.eval()\n",
    "      test_loss = 0\n",
    "      correct = 0\n",
    "      with torch.no_grad():\n",
    "          #print_idx = 0\n",
    "          for data, target in test_loader:\n",
    "              #print_idx = print_idx + 1; print('We are at {} index'.format(print_idx))\n",
    "              #data = data.double()\n",
    "              output = network(data)\n",
    "              test_loss += softmax_crossentropy_test(output, target).detach()\n",
    "              pred = output.data.max(1, keepdim=True)[1]\n",
    "              #ipdb.set_trace(context = 7)\n",
    "              correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "      test_loss /= len(test_loader.dataset)\n",
    "      test_losses.append(test_loss)\n",
    "      accc = 100. * correct / len(test_loader.dataset)\n",
    "      test_accuracy.append(accc)\n",
    "      print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "          test_loss, correct, len(test_loader.dataset),\n",
    "          100. * correct / len(test_loader.dataset)))\n",
    "  t1_test = time.time()\n",
    "  print('Started test before training')\n",
    "  test()\n",
    "  t2_test = time.time()\n",
    "  print('Finished test before training in {}s'.format(t2_test - t1_test))\n",
    "  print('\\n m_aa just after testing (should be empty if we did nto accumulate stats during traning - this si correct)', optimizer.m_aa, '\\n')\n",
    "  step_counter = 0\n",
    "  total_time_in_s = 0\n",
    "  total_time_in_s_list = []\n",
    "  for epoch in range(1, n_epochs + 1):\n",
    "      # SET HYPER-OPTIMIZER PARAMETERS ACCODRING TO SHCEDULE\n",
    "      if epoch in KFAC_matrix_update_frequency_dict:\n",
    "        optimizer.TCov =  KFAC_matrix_update_frequency_dict[epoch]\n",
    "      if epoch in KFAC_matrix_invert_frequency_dict:\n",
    "        optimizer.TInv = KFAC_matrix_invert_frequency_dict[epoch]\n",
    "      if epoch in KFAC_damping_dict: \n",
    "        optimizer.param_groups[0]['damping'] = KFAC_damping_dict[epoch]\n",
    "      if epoch in rsvd_rank_dict:\n",
    "        optimizer.rsvd_rank = rsvd_rank_dict[epoch]\n",
    "      if epoch in oversampling_parameter_dict:\n",
    "        optimizer.oversampling_parameter = oversampling_parameter_dict[epoch]\n",
    "      if epoch in rsvd_niter_dict:\n",
    "        optimizer.rsvd_niter = rsvd_niter_dict[epoch]\n",
    "\n",
    "      # END IMPLEMETING SCHEDULE for OPTIMIZER HYPERPARAMTER\n",
    "\n",
    "      t1 = time.time()\n",
    "      step_counter = train(epoch, step_counter)\n",
    "      t2 = time.time()\n",
    "    \n",
    "      test()\n",
    "      total_time_in_s = total_time_in_s + (t2-t1)\n",
    "      total_time_in_s_list.append(total_time_in_s)\n",
    "      ############ SAVE DATA #####################\n",
    "      save_data_()\n",
    "      np.save(os.path.join(basic_path, '{}_{}_{}_{}_{}_{}_run{}_{}'.format(opti_type, batch_size_train,\n",
    "                                                        beta1, beta2, l_rate_function(40,40), stat_decay, \n",
    "                                                        random_seed, 'total_train_time')),\n",
    "                    total_time_in_s_list)\n",
    "      ######### END SAVE DATA ####################\n",
    "      print('Total time: {}s'.format(total_time_in_s))\n",
    "  save_data = True\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2L0sci5tCv1J"
   },
   "outputs": [],
   "source": [
    "# ERROR DEBUGING\n",
    "#import numpy as np\n",
    "#A = np.load(error_write_path + '/m_aa_when_err.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lp3QWG-kIHrO"
   },
   "outputs": [],
   "source": [
    "#np.linalg.matrix_rank(A)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP57Db91r8BvH1TnTGJKPtG",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0fb3db7e606848eeafe5aa98acd5b1a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1adbeb91657d47e597da5da3a628755b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ce9f4cbce2b4ea390b166518e35bbb6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1adbeb91657d47e597da5da3a628755b",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c4501d3693e7402eacb8b7152e8ec503",
      "value": 170498071
     }
    },
    "1f55550f09cd4807975c968d48fab58f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2173101d247c45b7af7041ea918463c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f55550f09cd4807975c968d48fab58f",
      "placeholder": "​",
      "style": "IPY_MODEL_0fb3db7e606848eeafe5aa98acd5b1a9",
      "value": " 170498071/170498071 [00:01&lt;00:00, 112233409.98it/s]"
     }
    },
    "3401289cb1bf448a83bf556158a659e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "556071ce7018440794a344f9f6473b65": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7cca248bc93a4e5c93c4618cf081ebd7",
       "IPY_MODEL_1ce9f4cbce2b4ea390b166518e35bbb6",
       "IPY_MODEL_2173101d247c45b7af7041ea918463c7"
      ],
      "layout": "IPY_MODEL_cd913ed07cd64bc5941c77153062e141"
     }
    },
    "7cca248bc93a4e5c93c4618cf081ebd7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3401289cb1bf448a83bf556158a659e5",
      "placeholder": "​",
      "style": "IPY_MODEL_930a8d6c30a147f0b1e2bf918bc049c7",
      "value": "100%"
     }
    },
    "930a8d6c30a147f0b1e2bf918bc049c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c4501d3693e7402eacb8b7152e8ec503": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cd913ed07cd64bc5941c77153062e141": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
